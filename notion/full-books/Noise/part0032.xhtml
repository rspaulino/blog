<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0032</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_241" class="s6B">
<a href="part0003.xhtml#a2X1">CHAPTER 21</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2X1">Selection and Aggregation in Forecasting</a>
</div>
<div class="s4M">
<span class="s2WY">M</span>
any judgments involve forecasting. What is the unemployment rate likely to be in the next quarter? How many electric cars will be sold next year? What will be the effects of climate change in 2050? How long will it take to complete a new building? What will be the annual earnings of a particular company? How will a new employee perform? What will be the cost of a new air pollution regulation? Who will win an election? The answers to such questions have major consequences. Fundamental choices of private and public institutions often depend on them.</div>
<div class="s4P">Analysts of forecasting—of when it goes wrong and why—make a sharp distinction between bias and noise (also called inconsistency or unreliability). Everyone agrees that in some contexts, forecasters are biased. For example, official agencies show unrealistic optimism in their budget forecasts. On average, they project unrealistically high economic growth and unrealistically low deficits. For practical purposes, it matters little whether their unrealistic optimism is a product of a cognitive bias or political considerations.</div>
<div class="s4P">In addition, forecasters tend to be overconfident: if asked to formulate their forecasts as confidence intervals rather than as point estimates, they tend to pick narrower intervals than they should. For instance, an ongoing quarterly survey asks the chief financial officers of US companies to estimate the annual return of the S&amp;P 500 index for the next year. The CFOs provide two numbers: a minimum, below which they think there is a one-in-ten chance the actual return will be, and a maximum, which they believe the actual <span id="page_242"></span>
return has a one-in-ten chance of exceeding. Thus the two numbers are the bounds of an 80% confidence interval. Yet the realized returns fall in that interval only 36% of the time. The CFOs are far too confident in the precision of their forecasts.</div>
<div class="s4P">Forecasters are also noisy. A reference text, J. Scott Armstrong’s <span class="s2WT-0">Principles of Forecasting,</span>
 points out that even among experts, “unreliability is a source of error in judgmental forecasting.” In fact noise is a major source of error. Occasion noise is common; forecasters do not always agree with themselves. Between-person noise is also pervasive; forecasters disagree with one another, even if they are specialists. If you ask law professors to predict Supreme Court rulings, you will find a great deal of noise. If you ask specialists to project the annual benefits of air pollution regulation, you will find massive variability, with ranges of, for example, $3 billion to $9 billion. If you ask a group of economists to make forecasts about unemployment and growth, you will also find great variability. We have already seen many examples of noisy forecasts, and research on forecasting uncovers many more.</div>
<div class="s7D">Improving Forecasts</div>
<div class="s4M">The research also offers suggestions for reducing noise and bias. We will not review them exhaustively here, but we will focus on two noise-reduction strategies that have broad applicability. One is an application of the principle we mentioned in <a href="part0029.xhtml">chapter 18</a>
: selecting better judges produces better judgments. The other is one of the most universally applicable decision hygiene strategies: aggregating multiple independent estimates.</div>
<div class="s4P">The easiest way to aggregate several forecasts is to average them. Averaging is mathematically guaranteed to reduce noise: specifically, it divides it by the square root of the number of judgments averaged. This means that if you average one hundred judgments, you will reduce noise by 90%, and if you average four hundred judgments, you will reduce it by 95%—essentially eliminating it. This statistical law is the engine of the wisdom-of-crowds approach, discussed in <a href="part0015.xhtml">chapter 7</a>
.</div>
<div class="s4P">Because averaging does nothing to reduce bias, its effect on total error (MSE) depends on the proportions of bias and noise in it. <span id="page_243"></span>
That is why the wisdom of crowds works best when judgments are independent, and therefore less likely to contain shared biases. Empirically, ample evidence suggests that averaging multiple forecasts greatly increases accuracy, for instance in the “consensus” forecast of economic forecasters of stock analysts. With respect to sales forecasting, weather forecasting, and economic forecasting, the unweighted average of a group of forecasters outperforms most and sometimes all individual forecasts. Averaging forecasts obtained by different methods has the same effect: in an analysis of thirty empirical comparisons in diverse domains, combined forecasts reduced errors by an average of 12.5%.</div>
<div class="s4P">Straight averaging is not the only way to aggregate forecasts. A <span class="s2WT-0">select-crowd</span>
 strategy, which selects the best judges according to the accuracy of their recent judgments and averages the judgments of a small number of judges (e.g., five), can be as effective as straight averaging. It is also easier for decision makers who respect expertise to understand and adopt a strategy that relies not only on aggregation but also on selection.</div>
<div class="s4P">One method to produce aggregate forecasts is to use <span class="s2WT-0">prediction markets,</span>
 in which individuals bet on likely outcomes and are thus incentivized to make the right forecasts. Much of the time, prediction markets have been found to do very well, in the sense that if the prediction market price suggests that events are, say, 70% likely to happen, they happen about 70% of the time. Many companies in various industries have used prediction markets to aggregate diverse views.</div>
<div class="s4P">Another formal process for aggregating diverse views is known as the Delphi method. In its classic form, this method involves multiple rounds during which the participants submit estimates (or votes) to a moderator and remain anonymous to one another. At each new round, the participants provide reasons for their estimates and respond to the reasons given by others, still anonymously. The process encourages estimates to converge (and sometimes forces them to do so by requiring new judgments to fall within a specific range of the distribution of previous-round judgments). The method benefits both from aggregation and social learning.</div>
<div class="s4P">The Delphi method has worked well in many situations, but it can be challenging to implement. A simpler version, <span class="s2WT-0">mini-Delphi,</span>
 can be deployed within a single meeting. Also called <span id="page_244"></span>
<span class="s2WT-0">estimate-talk-estimate,</span>
 it requires participants first to produce separate (and silent) estimates, then to explain and justify them, and finally to make a new estimate in response to the estimates and explanations of others. The consensus judgment is the average of the individual estimates obtained in that second round.</div>
<div class="s7D">The Good Judgment Project</div>
<div class="s4M">Some of the most innovative work on the quality of forecasting, going well beyond what we have explored thus far, started in 2011, when three prominent behavioral scientists founded the Good Judgment Project. Philip Tetlock (whom we encountered in <a href="part0020.xhtml">chapter 11</a>
 when we discussed his assessment of long-term forecasts of political events); his spouse, Barbara Mellers; and Don Moore teamed up to improve our understanding of forecasting and, in particular, why some people are good at it.</div>
<div class="s4P">The Good Judgment Project started with the recruitment of tens of thousands of volunteers—not specialists or experts but ordinary people from many walks of life. They were asked to answer hundreds of questions, such as these:</div>
<div class="sJT"><img src="rsrc31K.jpg" alt="" class="sJS" />
 <span class="s2WT-3">Will North Korea detonate a nuclear device before the end of the year?</span>
</div>
<div class="sT9"><img src="rsrc31K.jpg" alt="" class="sJS" />
 <span class="s2WT-3">Will Russia officially annex additional Ukrainian territory in the next three months?</span>
</div>
<div class="sT9"><img src="rsrc31K.jpg" alt="" class="sJS" />
 <span class="s2WT-3">Will India or Brazil become a permanent member of the UN Security Council in the next two years?</span>
</div>
<div class="sJW"><img src="rsrc31K.jpg" alt="" class="sJS" />
 <span class="s2WT-3">In the next year, will any country withdraw from the eurozone?</span>
</div>
<div class="s4P">As these examples show, the project has focused on large questions about world events. Importantly, efforts to answer such questions raise many of the same problems that more mundane forecasts do. If a lawyer is asking whether a client will win in court, or if a television studio is asked whether a proposed show will be a big hit, forecasting skills are involved. Tetlock and his colleagues <span id="page_245"></span>
wanted to learn whether some people are especially good forecasters. They also wanted to learn whether the ability to forecast could be taught or at least improved.</div>
<div class="s4P">To understand the central findings, we need to explain some key aspects of the method adopted by Tetlock and his team to evaluate forecasters. First, they used a large number of forecasts, not just one or a few, where luck might be responsible for success or failure. If you predict that your favorite sports team will win its next game, and it does, you are not necessarily a good forecaster. Maybe you <span class="s2WT-0">always</span>
 predict that your favorite team will win: if that’s your strategy, and if they win only half the time, your forecasting ability is not especially impressive. To reduce the role of luck, the researchers examined how participants did, on average, across numerous forecasts.</div>
<div class="s4P">Second, the researchers asked participants to make their forecasts in terms of probabilities that an event would happen, rather than a binary “it will happen” or “it will not happen.” To many people, forecasting means the latter—taking a stand one way or the other. However, given our objective ignorance of future events, it is much better to formulate probabilistic forecasts. If someone said in 2016, “Hillary Clinton is 70% likely to be elected president,” he is not necessarily a bad forecaster. Things that are correctly said to be 70% likely will not happen 30% of the time. To know whether forecasters are good, we should ask whether their probability estimates map onto reality. Suppose that a particular forecaster named Margaret says that 500 different events are 60% likely. If 300 of them actually happen, then we can conclude that Margaret’s confidence is well <span class="s2WT-0">calibrated.</span>
 Good calibration is one requirement for good forecasting.</div>
<div class="s4P">Third, as an added refinement, Tetlock and colleagues did not just ask their forecasters to make <span class="s2WT-0">one</span>
 probability estimate about whether an event would happen in, say, twelve months. They gave the participants the opportunity to revise their forecasts continuously in light of new information. Suppose that you had estimated, back in 2016, that the United Kingdom had only a 30% chance of leaving the European Union before the end of 2019. As new polls came out, suggesting that the “Leave” vote was gaining ground, you probably would have revised your forecast upward. When the result of the referendum was known, it was still uncertain <span id="page_246"></span>
whether the United Kingdom would leave the union within that time frame, but it certainly looked a lot more probable. (Brexit technically happened in 2020.)</div>
<div class="s4P">With each new piece of information, Tetlock and his colleagues allowed the forecasters to update their forecasts. For scoring purposes, each one of these updates is treated as a new forecast. That way, participants in the Good Judgment Project are incentivized to monitor the news and update their forecasts continuously. This approach mirrors what is expected of forecasters in business and government, who should also be updating their forecasts frequently on the basis of new information, despite the risk of being criticized for changing their minds. (A well-known response to this criticism, sometimes attributed to John Maynard Keynes, is, “When the facts change, I change my mind. What do <span class="s2WT-0">you</span>
 do?”)</div>
<div class="s4P">Fourth, to score the performance of the forecasters, the Good Judgment Project used a system developed by Glenn W. Brier in 1950. <span class="s2WT-0">Brier scores,</span>
 as they are known, measure the distance between what people forecast and what actually happens.</div>
<div class="s4P">Brier scores are a clever way to get around a pervasive problem associated with probabilistic forecasts: the incentive for forecasters to hedge their bets by never taking a bold stance. Think again of Margaret, whom we described as a well-calibrated forecaster because she rated 500 events as 60% likely, and 300 of those events did happen. This result may not be as impressive as it seems. If Margaret is a weather forecaster who <span class="s2WT-0">always</span>
 predicts a 60% chance of rain and there are 300 rainy days out of 500, Margaret’s forecasts are well calibrated but also practically useless. Margaret, in essence, is telling you that, just in case, you might want to carry an umbrella every day. Compare her with Nicholas, who predicts a 100% chance of rain on the 300 days when it will rain, and a 0% chance of rain on the 200 dry days. Nicholas has the same perfect calibration as Margaret: when either forecaster predicts that X% of the days will be rainy, rain falls precisely X% of the time. But Nicholas’s forecasts are much more valuable: instead of hedging his bets, he is willing to tell you whether you should take an umbrella. Technically, Nicholas is said to have a high <span class="s2WT-0">resolution</span>
 in addition to good calibration.</div>
<div class="s4P">Brier scores reward both good calibration and good resolution. To produce a good score, you have not only to be right <span id="page_247"></span>
on average (i.e., well calibrated) but also to be willing to take a stand and differentiate among forecasts (i.e., have high resolution). Brier scores are based on the logic of mean squared errors, and lower scores are better: a score of 0 would be perfect.</div>
<div class="s4P">So, now that we know how they were scored, how well did the Good Judgment Project volunteers do? One of the major findings was that the overwhelming majority of the volunteers did poorly, but about 2% stood out. As mentioned earlier, Tetlock calls these well-performing people superforecasters. They were hardly unerring, but their predictions were much better than chance. Remarkably, one government official said that the group did significantly “better than the average for intelligence community analysts who could read intercepts and other secret data.” This comparison is worth pausing over. Intelligence community analysts are trained to make accurate forecasts; they are not amateurs. In addition, they have access to classified information. And yet they do not do as well as the superforecasters do.</div>
<div class="s7D">Perpetual Beta</div>
<div class="s4M">What makes superforecasters so good? Consistent with our argument in <a href="part0029.xhtml">chapter 18</a>
, we could reasonably speculate that they are unusually intelligent. That speculation is not wrong. On GMA tests, the superforecasters do better than the average volunteer in the Good Judgment Project (and the average volunteer is significantly above the national average). But the difference isn’t all that large, and many volunteers who do extremely well on intelligence tests do not qualify as superforecasters. Apart from general intelligence, we could reasonably expect that superforecasters are unusually good with numbers. And they are. But their real advantage is not their talent at math; it is their ease in thinking analytically and probabilistically.</div>
<div class="s4P">Consider superforecasters’ willingness and ability to structure and disaggregate problems. Rather than form a holistic judgment about a big geopolitical question (whether a nation will leave the European Union, whether a war will break out in a particular place, whether a public official will be assassinated), they break it up into its component parts. They ask, “What would it take <span id="page_248"></span>
for the answer to be yes? What would it take for the answer to be no?” Instead of offering a gut feeling or some kind of global hunch, they ask and try to answer an assortment of subsidiary questions.</div>
<div class="s4P">Superforecasters also excel at taking the outside view, and they care a lot about base rates. As explained for the Gambardi problem in <a href="part0023.xhtml">chapter 13</a>
, before you focus on the specifics of Gambardi’s profile, it helps to know the probability that the average CEO will be fired or quit in the next two years. Superforecasters systematically look for base rates. Asked whether the next year will bring an armed clash between China and Vietnam over a border dispute, superforecasters do not focus only or immediately on whether China and Vietnam are getting along right now. They might have an intuition about this, in light of the news and analysis they have read. But they know that their intuition about one event is generally not a good guide. Instead they start by looking for a base rate: they ask how often past border disputes have escalated into armed clashes. If such clashes are rare, superforecasters will begin by incorporating that fact and only then turn to the details of the China–Vietnam situation.</div>
<div class="s4P">In short, what distinguishes the superforecasters isn’t their sheer intelligence; it’s <span class="s2WT-0">how</span>
 they apply it. The skills they bring to bear reflect the sort of cognitive style we described in <a href="part0029.xhtml">chapter 18</a>
 as likely to result in better judgments, particularly a high level of “active open-mindedness.” Recall the test for actively open-minded thinking: it includes such statements as “People should take into consideration evidence that goes against their beliefs” and “It is more useful to pay attention to people who disagree with you than to pay attention to those who agree.” Clearly, people who score high on this test are not shy about updating their judgments (without overreacting) when new information becomes available.</div>
<div class="s4P">To characterize the thinking style of superforecasters, Tetlock uses the phrase “perpetual beta,” a term used by computer programmers for a program that is not meant to be released in a final version but that is endlessly used, analyzed, and improved. Tetlock finds that “the strongest predictor of rising into the ranks of superforecasters is perpetual beta, the degree to which one is committed to belief updating and self-improvement.” As he puts it, “What makes them so good is less what they are than what they do—the hard work of research, the careful thought and self-criticism, the <span id="page_249"></span>
gathering and synthesizing of other perspectives, the granular judgments and relentless updating.” They like a particular cycle of thinking: “try, fail, analyze, adjust, try again.”</div>
<div class="s7D">Noise and Bias in Forecasting</div>
<div class="s4M">At this point, you might be tempted to think that people can be trained to be superforecasters or at least to perform more like them. And indeed, Tetlock and his collaborators have worked to do exactly that. Their efforts should be considered the second stage of understanding why superforecasters perform so well and how to make them perform better.</div>
<div class="s4P">In an important study, Tetlock and his team randomly assigned regular (nonsuper) forecasters to three groups, in which they tested the effect of different interventions on the quality of subsequent judgments. These interventions exemplify three of the strategies we have described to improve judgments:</div>
<div class="s1M9">1. <span class="s2WT-0">Training:</span>
 Several forecasters completed a tutorial designed to improve their abilities by teaching them probabilistic reasoning. In the tutorial, the forecasters learned about various biases (including base-rate neglect, overconfidence, and confirmation bias); the importance of averaging multiple predictions from diverse sources; and considering reference classes.</div>
<div class="s1MB">2. <span class="s2WT-0">Teaming (a form of aggregation):</span>
 Some forecasters were asked to work in teams in which they could see and debate one another’s predictions. Teaming could increase accuracy by encouraging forecasters to deal with opposing arguments and to be actively open-minded.</div>
<div class="s1MD">3. <span class="s2WT-0">Selection:</span>
 All forecasters were scored for accuracy, and at the end of a full year, the top 2% were designated as superforecasters and given the opportunity to work together in elite teams the following year.</div>
<div class="s4P">As it turns out, all three interventions worked, in the sense <span id="page_250"></span>
that they improved people’s Brier scores. Training made a difference, teaming made a larger one, and selection had an even larger effect.</div>
<div class="s4P">This important finding confirms the value of aggregating judgments and selecting good judges. But it is not the full story. Armed with data about the effects of each intervention, Ville Satopää, who collaborated with Tetlock and Mellers, developed a sophisticated statistical technique to tease out how, exactly, each intervention improved forecasts. In principle, he reasoned, there are three major reasons why some forecasters can perform better or worse than others:</div>
<div class="s1M9">1. They can be more skilled at finding and analyzing data in the environment that are relevant to the prediction they have to make. This explanation points to the importance of information.</div>
<div class="s1MB">2. Some forecasters may have a general tendency to err on a particular side of the true value of a forecast. If, out of hundreds of forecasts, you systematically overestimate or underestimate the probability that certain changes from the status quo will occur, you can be said to suffer from a form of bias, in favor of either change or stability.</div>
<div class="s1MD">3. Some forecasters may be less susceptible to noise (or random errors). In forecasting, as in any judgment, noise can have many triggers. Forecasters may overreact to a particular piece of news (this is an example of what we have called pattern noise), they may be subject to occasion noise, or they may be noisy in their use of the probability scale. All these errors (and many more) are unpredictable in their size and direction.</div>
<div class="s4P">Satopää, Tetlock, Mellers, and their colleague Marat Salikhov called their model the BIN (bias, information, and noise) model for forecasting. They set out to measure how much each of the three components was responsible for the performance improvement in each of the three interventions.</div>
<div class="s4P">Their answer was simple: all three interventions worked primarily by reducing noise. As the researchers put it, “Whenever an <span id="page_251"></span>
intervention boosted accuracy, it worked mainly by suppressing random errors in judgment. Curiously, the original intent of the training intervention was to reduce bias.”</div>
<div class="s4P">Since the training was designed to reduce biases, a less-than-super forecaster would have predicted that bias reduction would be the major effect of the training. Yet the training worked by reducing noise. The surprise is easily explained. Tetlock’s training is designed to fight <span class="s2WT-0">psychological</span>
 biases. As you now know, the effect of psychological biases is not always a statistical bias. When they affect different individuals on different judgments in different ways, psychological biases produce noise. This is clearly the case here, as the events being forecast are quite varied. The same biases can lead a forecaster to overreact or underreact, depending on the topic. We should not expect them to produce a <span class="s2WT-0">statistical</span>
 bias, defined as the general tendency of a forecaster to believe that events will happen or not happen. As a result, training forecasters to fight their psychological biases works—by reducing noise.</div>
<div class="s4P">Teaming had a comparably large effect on noise reduction, but it also significantly improved the ability of the teams to extract information. This result is consistent with the logic of aggregation: several brains that work together are better at finding information than one is. If Alice and Brian are working together, and Alice has spotted signals that Brian has missed, their joint forecast will be better. When working in groups, the superforecasters seem capable of avoiding the dangers of group polarization and information cascades. Instead, they pool their data and insights and, in their actively open-minded way, make the most of the combined information. Satopää and his colleagues explain this advantage: “Teaming—unlike training… allows forecasters to harness the information.”</div>
<div class="s4P">Selection had the largest total effect. Some of the improvement comes from a better use of information. Superforecasters are better than others at finding relevant information—possibly because they are smarter, more motivated, and more experienced at making these kinds of forecasts than is the average participant. But the main effect of selection is, again, to reduce noise. Superforecasters are less noisy than regular players or even trained teams. This finding, too, was a surprise to Satopää and the other researchers: “ ‘Superforecasters’ may owe their success <span id="page_252"></span>
more to superior discipline in tamping down measurement error, than to incisive readings of the news” that others cannot replicate.</div>
<div class="s7D">Where Selection and Aggregation Work</div>
<div class="s4M">The success of the superforecasting project highlights the value of two decision hygiene strategies: <span class="s2WT-0">selection</span>
 (the superforecasters are, well, super) and <span class="s2WT-0">aggregation</span>
 (when they work in teams, forecasters perform better). The two strategies are broadly applicable in many judgments. Whenever possible, you should aim to combine the strategies, by constructing teams of judges (e.g., forecasters, investment professionals, recruiting officers) who are selected for being both good at what they do <span class="s2WT-0">and</span>
 complementary to one another.</div>
<div class="s4P">So far, we have considered the improved precision that is achieved by averaging multiple independent judgments, as in the wisdom-of-crowds experiments. Aggregating the estimates of higher-validity judges will further improve accuracy. Yet another gain in accuracy can be obtained by combining judgments that are both independent and complementary. Imagine that four people are witnesses to a crime: it is essential, of course, to make sure that they do not influence one another. If, in addition, they have seen the crime from four different angles, the quality of the information they provide will be much better.</div>
<div class="s4P">The task of assembling a team of professionals to make judgments together resembles the task of assembling a battery of tests to predict the future performance of candidates at school or on the job. The standard tool for that task is multiple regression (introduced in <a href="part0018.xhtml">chapter 9</a>
). It works by selecting variables in succession. The test that best predicts the outcome is selected first. However, the next test to be included is not necessarily the second most valid. Instead, it is the one that <span class="s2WT-0">adds</span>
 the most predictive power to the first test, by providing predictions that are both valid and not redundant with the first. For example, suppose you have two tests of mental aptitude, which correlate .50 and .45 with future performance, and a test of personality that correlates only .30 with performance but is uncorrelated with aptitude tests. The optimal solution is to pick the more valid aptitude test first, then the <span id="page_253"></span>
personality test, which brings more new information.</div>
<div class="s4P">Similarly, if you are assembling a team of judges, you should of course pick the best judge first. But your next choice may be a moderately valid individual who brings some new skill to the table rather than a more valid judge who is highly similar to the first one. A team selected in this manner will be superior because the validity of pooled judgments increases faster when the judgments are uncorrelated with one another than when they are redundant. Pattern noise will be relatively high in such a team because individual judgments of each case will differ. Paradoxically, the average of that noisy group will be more accurate than the average of a unanimous one.</div>
<div class="s4P">An important caveat is in order. Regardless of diversity, aggregation can only reduce noise if judgments are truly independent. As our discussion of noise in groups has highlighted, group deliberation often adds more error in bias than it removes in noise. Organizations that want to harness the power of diversity must welcome the disagreements that will arise when team members reach their judgments independently. Eliciting and aggregating judgments that are both independent and diverse will often be the easiest, cheapest, and most broadly applicable decision hygiene strategy.</div>
<div class="s84">Speaking of Selection and Aggregation</div>
<div class="s86">
<span class="class-2">“Let’s take the average of four independent judgments—this is guaranteed to reduce noise by half.”</span>
</div>
<div class="s88">
<span class="class-2">“We should strive to be in perpetual beta, like the superforecasters.”</span>
</div>
<div class="s88">
<span class="class-2">“Before we discuss this situation, what is the relevant base rate?”</span>
</div>
<div class="s8A">
<span class="class-2">“We have a good team, but how can we ensure more diversity of opinions?”</span>
</div>
</body>
</html>
