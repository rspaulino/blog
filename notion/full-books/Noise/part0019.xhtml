<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0019</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_118" class="s6B">
<a href="part0003.xhtml#a2Y4">CHAPTER 10</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2Y4">Noiseless Rules</a>
</div>
<div class="s4M">
<span class="s2WY">I</span>
n recent years, artificial intelligence (AI), particularly machine-learning techniques, has enabled machines to perform many tasks formerly regarded as quintessentially human. Machine-learning algorithms can recognize faces, translate languages, and read radiology images. They can solve computational problems, such as generating driving directions for thousands of drivers at once, with astonishing speed and accuracy. And they perform difficult prediction tasks: machine-learning algorithms forecast the decisions of the US Supreme Court, determine which defendants are more likely to jump bail, and assess which calls to child protective services most urgently require a case worker’s visit.</div>
<div class="s4P">Although nowadays these are the applications we have in mind when we hear the word <span class="s2WT-0">algorithm,</span>
 the term has a broader meaning. In one dictionary’s definition, an algorithm is a “process or set of rules to be followed in calculations or other problem-solving operations, especially by a computer.” By this definition, simple models and other forms of mechanical judgment we described in the previous chapter are algorithms, too.</div>
<div class="s4P">In fact, many types of mechanical approaches, from almost laughably simple rules to the most sophisticated and impenetrable machine algorithms, can outperform human judgment. And one key reason for this outperformance—albeit not the only one—is that all mechanical approaches are noise-free.</div>
<div class="s4P">To examine different types of rule-based approaches and to learn how and under what conditions each approach can be valuable, we start our journey with the models of <a href="part0018.xhtml">chapter 9</a>
: simple models based on multiple regression (i.e., linear regression models). From this starting point, we will travel in the two opposite directions <span id="page_119"></span>
on the spectrum of sophistication—first to seek extreme simplicity, then to add greater sophistication (<a href="part0019.xhtml#a2ZE">figure 11</a>
).</div>
<div class="sE6">
<div class="sK">
<div class="sH-0"><img src="rsrc31M.jpg" alt="" id="a2ZE" class="sH-1" />
</div>
<div class="sE3">F<span class="s2WW">IGURE 11:</span>
 <span class="s2WT-0">Four types of rules and algorithms</span>
</div>
</div>
</div>
<div class="s7D">More Simplicity: Robust and Beautiful</div>
<div class="s4M">Robyn Dawes was another member of the Eugene, Oregon, team of stars that studied judgment in the 1960s and 1970s. In 1974, Dawes achieved a breakthrough in the simplification of prediction tasks. His idea was surprising, almost heretical: instead of using multiple regression to determine the precise weight of each predictor, he proposed giving all the predictors equal weights.</div>
<div class="s4P">Dawes labeled the equal-weight formula an <span class="s2WT-0">improper linear model</span>
. His surprising discovery was that these equal-weight models are about as accurate as “proper” regression models, and far superior to clinical judgments.</div>
<div class="s4P">Even the proponents of improper models admit that this claim is implausible and “contrary to statistical intuition.” Indeed, Dawes and his assistant, Bernard Corrigan, initially struggled to publish their paper in scientific journals; editors simply did not believe them. If you think about the example of Monica and Nathalie in the previous chapter, you probably believe that some predictors matter more than others. Most people, for instance, would give leadership a higher weight than technical skills. How can a straight unweighted average predict someone’s performance better than a carefully weighted average, or better than the judgment of an expert?</div>
<div class="s4P">Today, many years after Dawes’s breakthrough, the statistical phenomenon that so surprised his contemporaries is well understood. As explained earlier in this book, multiple regression computes “optimal” weights that minimize squared errors. But multiple regression minimizes error <span class="s2WT-0">in the original data.</span>
 The formula therefore adjusts itself to predict every random fluke in the data. If, for instance, the sample includes a few managers who have <span id="page_120"></span>
high technical skills and who also performed exceptionally well for unrelated reasons, the model will exaggerate the weight of technical skill.</div>
<div class="s4P">The challenge is that when the formula is applied <span class="s2WT-0">out of sample</span>
—that is, when it is used to predict outcomes in a different data set—the weights will no longer be optimal. Flukes in the original sample are no longer present, precisely because they were flukes; in the new sample, managers with high technical skills are not all superstars. And the new sample has different flukes, which the formula cannot predict. The correct measure of a model’s predictive accuracy is its performance in a new sample, called its <span class="s2WT-0">cross-validated correlation</span>
. In effect, a regression model is <span class="s2WT-0">too</span>
 successful in the original sample, and a cross-validated correlation is almost always lower than it was in the original data. Dawes and Corrigan compared equal-weight models to multiple regression models (cross-validated) in several situations. One of their examples involved predictions of the first-year GPA of ninety graduate students in psychology at the University of Illinois, using ten variables related to academic success: aptitude test scores, college grades, various peer ratings (e.g., extroversion), and various self-ratings (e.g., conscientiousness). The standard multiple regression model achieved a correlation of .69, which shrank to .57 (PC = 69%) in cross-validation. The correlation of the equal-weight model with first-year GPA was about the same: .60 (PC = 70%). Similar results have been obtained in many other studies.</div>
<div class="s4P">The loss of accuracy in cross-validation is worst when the original sample is small, because flukes loom larger in small samples. The problem Dawes pointed out is that the samples used in social science research are generally so small that the advantage of so-called optimal weighting disappears. As statistician Howard Wainer memorably put it in the subtitle of a scholarly article on the estimation of proper weights, “It Don’t Make No Nevermind.” Or, in Dawes’s words, “we do not need models more precise than our measurements.” Equal-weight models do well because they are not susceptible to accidents of sampling.</div>
<div class="s4P">The immediate implication of Dawes’s work deserves to be widely known: you can make valid statistical predictions without prior data about the outcome that you are trying to predict. All you need is a collection of predictors that you can trust to be correlated <span id="page_121"></span>
with the outcome.</div>
<div class="s4P">Suppose you must make predictions of the performance of executives who have been rated on a number of dimensions, as in the example in <a href="part0018.xhtml">chapter 9</a>
. You trust that these scores measure important qualities, but you have no data about how well each score predicts performance. Nor do you have the luxury of waiting a few years to track the performance of a large sample of managers. You could nevertheless take the seven scores, do the statistical work required to weight them equally, and use the result as your prediction. How good would this equal-weight model be? Its correlation with the outcome would be .25 (PC = 58%), far superior to clinical predictions (<span class="s2WT-0">r</span>
 = .15, PC = 55%), and surely quite similar to a cross-validated regression model. And it does not require any data you don’t have or any complicated calculations.</div>
<div class="s4P">To use Dawes’s phrase, which has become a meme among students of judgment, there is a “robust beauty” in equal weights. The final sentence of the seminal article that introduced the idea offered another pithy summary: “The whole trick is to decide what variables to look at and then to know how to add.”</div>
<div class="s7D">Even More Simplicity: Simple Rules</div>
<div class="s4M">Another style of simplification is through <span class="s2WT-0">frugal models,</span>
 or <span class="s2WT-0">simple rules.</span>
 Frugal models are models of reality that look like ridiculously simplified, back-of-the-envelope calculations. But in some settings, they can produce surprisingly good predictions.</div>
<div class="s4P">These models build on a feature of multiple regression that most people find surprising. Suppose you are using two predictors that are strongly predictive of the outcome—their correlations with the outcome are .60 (PC = 71%) and .55 (PC = 69%). Suppose also that the two predictors are correlated to each other, with a correlation of .50. How good would you guess your prediction is going to be when the two predictors are optimally combined? The answer is quite disappointing. The correlation is .67 (PC = 73%), higher than before, but not much higher.</div>
<div class="s4P">The example illustrates a general rule: the combination of two or more correlated predictors is barely more predictive than the best of them on its own. Because, in real life, predictors are <span id="page_122"></span>
almost always correlated to one another, this statistical fact supports the use of frugal approaches to prediction, which use a small number of predictors. Simple rules that can be applied with little or no computation have produced impressively accurate predictions in some settings, compared with models that use many more predictors.</div>
<div class="s4P">A team of researchers published in 2020 a large-scale effort to apply a frugal approach to a variety of prediction problems, including the choice that bail judges face when they decide whether to release or retain defendants pending trial. That decision is an implicit prediction of the defendant’s behavior. If wrongly denied bail, that person will be detained needlessly, at a significant cost to the individual and to society. If bail is granted to the wrong defendant, the person may flee before trial or even commit another crime.</div>
<div class="s4P">The model that the researchers built uses just two inputs known to be highly predictive of a defendant’s likelihood to jump bail: the defendant’s age (older people are lower flight risks) and the number of past court dates missed (people who have failed to appear before tend to recidivate). The model translates these two inputs into a number of points, which can be used as a risk score. The calculation of risk for a defendant does not require a computer—in fact, not even a calculator.</div>
<div class="s4P">When tested against a real data set, this frugal model performed as well as statistical models that used a much larger number of variables. The frugal model did better than virtually all human bail judges did in predicting flight risk.</div>
<div class="s4P">The same frugal approach, using up to five features weighted by small whole numbers (between −3 and +3), was applied to tasks as varied as determining the severity of a tumor from mammographic data, diagnosing heart disease, and predicting credit risk. In all these tasks, the frugal rule did as well as more complex regression models did (though generally not as well as machine learning did).</div>
<div class="s4P">In another demonstration of the power of simple rules, a separate team of researchers studied a similar but distinct judicial problem: recidivism prediction. Using only two inputs, they were able to match the validity of an existing tool that uses 137 variables to assess a defendant’s risk level. Not surprisingly, these two <span id="page_123"></span>
predictors (age and the number of previous convictions) are closely related to the two factors used in the bail model, and their association with criminal behavior is well documented.</div>
<div class="s4P">The appeal of frugal rules is that they are transparent and easy to apply. Moreover, these advantages are obtained at relatively little cost in accuracy relative to more complex models.</div>
<div class="s7D">More Complexity: Toward Machine Learning</div>
<div class="s4M">For the second part of our journey, let us now travel in the opposite direction on the spectrum of sophistication. What if we could use many more predictors, gather much more data about each of them, spot relationship patterns that no human could detect, and model these patterns to achieve better prediction? This, in essence, is the promise of AI.</div>
<div class="s4P">Very large data sets are essential for sophisticated analyses, and the increasing availability of such data sets is one of the main causes of the rapid progress of AI in recent years. For example, large data sets make it possible to deal mechanically with <span class="s2WT-0">broken-leg exceptions.</span>
 This somewhat cryptic phrase goes back to an example that Meehl imagined: Consider a model that was designed to predict the probability that people will go to the movies tonight. Regardless of your confidence in the model, if you happen to know that a particular person just broke a leg, you probably know better than the model what their evening will look like.</div>
<div class="s4P">When using simple models, the broken-leg principle holds an important lesson for decision makers: it tells them when to override the model and when not to. If you have decisive information that the model could not take into consideration, there is a true broken leg, and you should override the model’s recommendation. On the other hand, you will sometimes disagree with a model’s recommendation even if you lack such private information. In those cases, your temptation to override the model reflects a personal pattern you are applying to the same predictors. Since this personal pattern is highly likely to be invalid, you should refrain from overriding the model; your intervention is likely to make the prediction less accurate.</div>
<div class="s4P">One of the reasons for the success of machine-learning models in prediction tasks is that they are capable of discovering <span id="page_124"></span>
such broken legs—many more than humans can think of. Given a vast amount of data about a vast number of cases, a model tracking the behavior of moviegoers could actually learn, for example, that people who have visited the hospital on their regular movie day are unlikely to see a film that evening. Improving predictions of rare events in this way reduces the need for human supervision.</div>
<div class="s4P">What AI does involves no magic and no understanding; it is mere pattern finding. While we must admire the power of machine learning, we should remember that it will probably take some time for an AI to understand <span class="s2WT-0">why</span>
 a person who has broken a leg will miss movie night.</div>
<div class="s7D">An Example: Better Bail Decisions</div>
<div class="s4M">At about the same time that the previously mentioned team of researchers applied simple rules to the problem of bail decisions, another team, led by Sendhil Mullainathan, trained sophisticated AI models to perform the same task. The AI team had access to a bigger set of data—758,027 bail decisions. For each case, the team had access to information also available to the judge: the defendant’s current offense, rap sheet, and prior failures to appear. Except for age, no other demographic information was used to train the algorithm. The researchers also knew, for each case, whether the defendant was released and, if so, whether the individual failed to appear in court or was rearrested. (Of the defendants, 74% were released, and of these, 15% failed to appear in court and 26% were rearrested.) With this data, the researchers trained a machine-learning algorithm and evaluated its performance. Since the model was built through machine learning, it was not restricted to linear combinations. If it detected a more complex regularity in the data, it could use this pattern to improve its predictions.</div>
<div class="s4P">The model was designed to produce a prediction of flight risk quantified as a numerical score, rather than a bail/no-bail decision. This approach recognizes that the maximum acceptable risk threshold, that is, the level of risk above which a defendant should be denied bail, requires an evaluative judgment that a model cannot make. However, the researchers calculated that, no matter where the risk threshold is set, using their model’s predictive score would <span id="page_125"></span>
result in improvements over the performance of human judges. If the risk threshold is set so that the number of people who are denied bail remains the same as when the judges decide, Mullainathan’s team calculated, crime rates could be reduced by up to 24%, because the people behind bars would be the ones most likely to recidivate. Conversely, if the risk threshold is set to reduce the number of people denied bail as much as possible without increasing crime, the researchers calculated that the number of people detained could be reduced by up to 42%. In other words, the machine-learning model performs much better than human judges do at predicting which defendants are high risks.</div>
<div class="s4P">The model built by machine learning was also far more successful than linear models that used the same information. The reason is intriguing: “The machine-learning algorithm finds significant signal in combinations of variables that might otherwise be missed.” The algorithm’s ability to find patterns easily missed by other methods is especially pronounced for the defendants whom the algorithm classifies as highest risk. In other words, some patterns in the data, though rare, strongly predict high risk. This finding—that the algorithm picks up rare but decisive patterns—brings us back to the concept of broken legs.</div>
<div class="s4P">The researchers also used the algorithm to build a model of each judge, analogous to the model of the judge we described in <a href="part0018.xhtml">chapter 9</a>
 (but not restricted to simple linear combinations). Applying these models to the entire set of data enabled the team to simulate the decisions judges would have made if they had seen the same cases, and to compare the decisions. The results indicated considerable system noise in bail decisions. Some of it is level noise: when judges are sorted by leniency, the most lenient quintile (that is, the 20% of judges who have the highest release rates) released 83% of the defendants, whereas the least lenient quintile of judges released only 61%. Judges also have very different patterns of judgments about which defendants are higher flight risks. A defendant who is seen as a low flight risk by one judge can be considered a high flight risk by another judge, who is not stricter in general. These results offer clear evidence of pattern noise. A more detailed analysis revealed that differences between cases accounted for 67% of the variance, and system noise for 33%. System noise included some level noise, i.e., differences in average severity, but <span id="page_126"></span>
most of it (79%) was pattern noise.</div>
<div class="s4P">Finally, and fortunately, the greater accuracy of the machine-learning program does not come at the expense of other identifiable goals that the judges might have pursued—notably, racial fairness. In theory, although the algorithm uses no racial data, the program might inadvertently aggravate racial disparities. These disparities could arise if the model used predictors that are highly correlated with race (such as zip code) or if the source of the data on which the algorithm is trained is biased. If, for instance, the number of past arrests is used as a predictor, and if past arrests are affected by racial discrimination, then the resulting algorithm will discriminate as well.</div>
<div class="s4P">While this sort of discrimination is certainly a risk in principle, the decisions of this algorithm are in important respects less racially biased than those of the judges, not more. For instance, if the risk threshold is set to achieve the same crime rate as the judges’ decisions did, then the algorithm jails 41% fewer people of color. Similar results are found in other scenarios: the gains in accuracy need not exacerbate racial disparities—and as the research team also showed, the algorithm can easily be instructed to reduce them.</div>
<div class="s4P">Another study in a different domain illustrates how algorithms can simultaneously increase accuracy and reduce discrimination. Bo Cowgill, a professor at Columbia Business School, studied the recruitment of software engineers at a large tech company. Instead of using (human) résumé screeners to select who would get an interview, Cowgill developed a machine-learning algorithm to screen the résumés of candidates and trained it on more than three hundred thousand submissions that the company had received and evaluated. Candidates selected by the algorithm were 14% more likely than those selected by humans to receive a job offer after interviews. When the candidates received offers, the algorithm group was 18% more likely than the human-selected group to accept them. The algorithm also picked a more diverse group of candidates, in terms of race, gender, and other metrics; it was much more likely to select “nontraditional” candidates, such as those who did not graduate from an elite school, those who lacked prior work experience, and those who did not have a referral. Human beings tended to favor résumés that checked all the boxes of the “typical” <span id="page_127"></span>
profile for a software engineer, but the algorithm gave each relevant predictor its proper weight.</div>
<div class="s4P">To be clear, these examples do not prove that algorithms are always fair, unbiased, or nondiscriminatory. A familiar example is an algorithm that is supposed to predict the success of job candidates, but is actually trained on a sample of past promotion decisions. Of course, such an algorithm will replicate all the human biases in past promotion decisions.</div>
<div class="s4P">It is possible, and perhaps too easy, to build an algorithm that perpetuates racial or gender disparities, and there have been many reported cases of algorithms that did just that. The visibility of these cases explains the growing concern about bias in algorithmic decision making. Before drawing general conclusions about algorithms, however, we should remember that some algorithms are not only more accurate than human judges but also fairer.</div>
<div class="s7D">Why Don’t We Use Rules More Often?</div>
<div class="s4M">To summarize this short tour of mechanical decision making, we review two reasons for the superiority of rules of all kinds over human judgment. First, as described in <a href="part0018.xhtml">chapter 9</a>
, all mechanical prediction techniques, not just the most recent and more sophisticated ones, represent significant improvements on human judgment. The combination of personal patterns and occasion noise weighs so heavily on the quality of human judgment that simplicity and noiselessness are sizable advantages. Simple rules that are merely sensible typically do better than human judgment.</div>
<div class="s4P">Second, the data is sometimes rich enough for sophisticated AI techniques to detect valid patterns and go well beyond the predictive power of a simple model. When AI succeeds in this way, the advantage of these models over human judgment is not just the absence of noise but also the ability to exploit much more information.</div>
<div class="s4P">Given these advantages and the massive amount of evidence supporting them, it is worth asking why algorithms are not used much more extensively for the types of professional judgments we discuss in this book. For all the spirited talk about algorithms and machine learning, and despite important exceptions in particular <span id="page_128"></span>
fields, their use remains limited. Many experts ignore the clinical-versus-mechanical debate, preferring to trust their judgment. They have faith in their intuitions and doubt that machines could do better. They regard the idea of algorithmic decision making as dehumanizing and as an abdication of their responsibility.</div>
<div class="s4P">The use of algorithms in medical diagnosis, for instance, is not yet routine, notwithstanding impressive advances. Few organizations use algorithms in their hiring and promotion decisions. Hollywood studio executives green-light movies on the basis of their judgment and experience, not according to a formula. Book publishers do the same thing. And if the tale of the statistics-obsessed Oakland Athletics baseball team, as told in Michael Lewis’s bestseller <span class="s2WT-0">Moneyball,</span>
 has made such an impression, it is precisely because algorithmic rigor had long been the exception, not the rule, in the decision-making process of sports teams. Even today, coaches, managers, and people who work with them often trust their gut and insist that statistical analysis cannot possibly replace good judgment.</div>
<div class="s4P">In a 1996 article, Meehl and a coauthor listed (and rebutted) no fewer than seventeen types of objections that psychiatrists, physicians, judges, and other professionals had to mechanical judgment. The authors concluded that the resistance of clinicians can be explained by a combination of sociopsychological factors, including their “fear of technological unemployment,” “poor education,” and a “general dislike of computers.”</div>
<div class="s4P">Since then, researchers have identified additional factors that contribute to this resistance. We do not aim to offer a full review of that research here. Our goal in this book is to offer suggestions for the improvement of human judgment, not to argue for the “displacement of people by machines,” as Judge Frankel would have put it.</div>
<div class="s4P">But some findings about what drives human resistance to mechanical prediction are relevant to our discussion of human judgment. One key insight has emerged from recent research: people are not systematically suspicious of algorithms. When given a choice between taking advice from a human and an algorithm, for instance, they often prefer the algorithm. Resistance to algorithms, or <span class="s2WT-0">algorithm aversion,</span>
 does not always manifest itself in a blanket refusal to adopt new decision support tools. More often, people are <span id="page_129"></span>
willing to give an algorithm a chance but stop trusting it as soon as they see that it makes mistakes.</div>
<div class="s4P">On one level, this reaction seems sensible: why bother with an algorithm you can’t trust? As humans, we are keenly aware that we make mistakes, but that is a privilege we are not prepared to share. We expect machines to be perfect. If this expectation is violated, we discard them.</div>
<div class="s4P">Because of this intuitive expectation, however, people are likely to distrust algorithms and keep using their judgment, even when this choice produces demonstrably inferior results. This attitude is deeply rooted and unlikely to change until near-perfect predictive accuracy can be achieved.</div>
<div class="s4P">Fortunately, much of what makes rules and algorithms better can be replicated in human judgment. We cannot hope to use information as efficiently as an AI model does, but we can strive to emulate the simplicity and noiselessness of simple models. To the extent that we can adopt methods that reduce system noise, we should see improvements in the quality of predictive judgments. How to improve our judgments is the main theme of <a href="part0028.xhtml">part 5</a>
.</div>
<div class="s84">Speaking of Rules and Algorithms</div>
<div class="s86">
<span class="class-2">“When there is a lot of data, machine-learning algorithms will do better than humans and better than simple models. But even the simplest rules and algorithms have big advantages over human judges: they are free of noise, and they do not attempt to apply complex, usually invalid insights about the predictors.”</span>
</div>
<div class="s88">
<span class="class-2">“Since we lack data about the outcome we must predict, why don’t we use an equal-weight model? It will do almost as well as a proper model, and will surely do better than case-by-case human judgment.”</span>
</div>
<div class="s88">
<span class="class-2">“You disagree with the model’s forecast. I get it. But is there a broken leg here, or do you just dislike the prediction?”</span>
</div>
<div class="s8A">
<span class="class-2">“The algorithm makes mistakes, of course. But if human judges make even more mistakes, whom should we trust?”</span>
</div>
</body>
</html>
