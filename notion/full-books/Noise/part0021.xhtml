<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0021</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_140" class="s6B">
<a href="part0003.xhtml#a2YN">CHAPTER 12</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2YN">The Valley of the Normal</a>
</div>
<div class="s4M">
<span class="s2WY">W</span>
e now turn to a broader question: how do we achieve comfort in a world in which many problems are easy but many others are dominated by objective ignorance? After all, where objective ignorance is severe, we should, after a while, become aware of the futility of crystal balls in human affairs. But that is not our usual experience of the world. Instead, as the previous chapter suggested, we maintain an unchastened willingness to make bold predictions about the future from little useful information. In this chapter, we address the prevalent and misguided sense that events that could not have been predicted can nevertheless be understood.</div>
<div class="s4P">What does this belief really mean? We raise that question in two contexts: the conduct of social science and the experience of the events of daily life.</div>
<div class="s7D">Predicting Life Trajectories</div>
<div class="s4M">In 2020, a group of 112 researchers, led by Sara McLanahan and Matthew Salganik, both professors of sociology at Princeton University, published an unusual article in the <span class="s2WT-0">Proceedings of the National Academy of Sciences.</span>
 The researchers aimed to figure out how much social scientists actually understand about what will happen in the life trajectories of socially fragile families. Knowing what they know, how well can social scientists predict events in a family’s life? Specifically, what level of accuracy can experts achieve when predicting life events, using the information that sociologists normally collect and apply in their research? In our terms, the aim of the study was to measure the level of objective ignorance that <span id="page_141"></span>
remains in these life events after sociologists have done their work.</div>
<div class="s4P">The authors drew material from the Fragile Families and Child Wellbeing Study, a large-scale longitudinal investigation of children who were followed from birth to fifteen years of age. The huge database contains several thousand items of information about the families of almost five thousand children, most of them born to unmarried parents in large US cities. The data covers topics such as the education and employment of the child’s grandparents, details about the health of all family members, indices of economic and social status, answers to multiple questionnaires, and tests of cognitive aptitude and personality. This is an extraordinary wealth of information, and social scientists have made good use of it: more than 750 scientific articles have been written based on data from the Fragile Families study. Many of these papers used the background data about children and their families to explain life outcomes such as high school grades and criminal record.</div>
<div class="s4P">The study led by the Princeton team focused on the predictability of six outcomes observed when the child was fifteen years old, including the occurrence of a recent eviction, the child’s GPA, and a general measure of the household’s material circumstances. The organizers used what they called the “common task method.” They invited teams of researchers to compete in generating accurate predictions of the six chosen outcomes, using the mass of data available about each family in the Fragile Families study. This type of challenge is novel in the social sciences but common in computer science, where teams are often invited to compete in tasks such as machine translation of a standard set of texts or detection of an animal in a large set of photographs. The achievement of the winning team in these competitions defines the state of the art at a point in time, which is always exceeded in the next competition. In a social science prediction task, where rapid improvement is not expected, it is reasonable to use the most accurate prediction achieved in the competition as a measure of the predictability of the outcome from these data—in other words, the residual level of objective ignorance.</div>
<div class="s4P">The challenge evoked considerable interest among researchers. The final report presented results from 160 highly qualified teams drawn from a much larger international pool of applicants. Most of the selected competitors described themselves <span id="page_142"></span>
as data scientists and used machine learning.</div>
<div class="s4P">In the first stage of the competition, the participating teams had access to all the data for half of the total sample; the data included the six outcomes. They used this “training data” to train a predictive algorithm. Their algorithms were then applied to a holdout sample of families that had not been used to train the algorithm. The researchers measured accuracy using MSE: the prediction error for each case was the square of the difference between the real outcome and the algorithm’s prediction.</div>
<div class="s4P">How good were the winning models? The sophisticated machine-learning algorithms trained on a large data set did, of course, outperform the predictions of simple linear models (and would, by extension, out-predict human judges). But the improvement the AI models delivered over a very simple model was slight, and their predictive accuracy remained disappointingly low. When predicting evictions, the best model achieved a correlation of .22 (PC = 57%). Similar results were found for other single-event outcomes, such as whether the primary caregiver had been laid off or had been in job training and how the child would score on a self-reported measure of “grit,” a personality trait that combines perseverance and passion for a particular goal. For these, the correlations fell between .17 and .24 (PC = 55 – 58%).</div>
<div class="s4P">Two of the six target outcomes were aggregates, which were much more predictable. The predictive correlations were .44 (PC = 65%) with the child’s GPA, and .48 (PC = 66%) with a summary measure of material hardship during the preceding twelve months. This measure was based on eleven questions, including “Were you ever hungry?” and “Was your telephone service canceled?” Aggregate measures are widely known to be both more predictive and more predictable than are measures of single outcomes. The main conclusion of the challenge is that a large mass of predictive information does not suffice for the prediction of single events in people’s lives—and even the prediction of aggregates is quite limited.</div>
<div class="s4P">The results observed in this research are typical, and many correlations that social scientists report fall in this range. An extensive review of research in social psychology, covering 25,000 studies and involving 8 million subjects over one hundred years, concluded that “social psychological effects typically yield a value of <span class="s2WT-0">r</span>
 [correlation coefficient] equal to .21.” Much higher correlations, <span id="page_143"></span>
like the .60 we mentioned earlier between adult height and foot size, are common for physical measurements but are very rare in the social sciences. A review of 708 studies in the behavioral and cognitive sciences found that only 3% of reported correlations were .50 or more.</div>
<div class="s4P">Such low correlation coefficients may come as a surprise if you are used to reading about findings that are presented as “statistically significant” or even “highly significant.” Statistical terms are often misleading to the lay reader, and “significant” may be the worst example of this. When a finding is described as “significant,” we should not conclude that the effect it describes is a strong one. It simply means that the finding is unlikely to be the product of chance alone. With a sufficiently large sample, a correlation can be at once very “significant” and too small to be worth discussing.</div>
<div class="s4P">The limited predictability of single outcomes in the challenge study carries a troubling message about the difference between understanding and prediction. The Fragile Families study is considered a treasure trove of social science, and as we have seen, its data has been used in a vast body of research. The scholars who produced that research surely felt that their work advanced the understanding of the lives of fragile families. Unfortunately, this sense of progress was not matched by an ability to make granular predictions about individual events in individual lives. The introductory abstract of the multiauthored report on the Fragile Families challenge contained a stark admonition: “Researchers must reconcile the idea that they understand life trajectories with the fact that none of the predictions were very accurate.”</div>
<div class="s7D">Understanding and Prediction</div>
<div class="s4M">The logic behind this pessimistic conclusion requires some elaboration. When the authors of the Fragile Families challenge equate understanding with prediction (or the absence of one with the absence of the other), they use the term <span class="s2WT-0">understanding</span>
 in a specific sense. There are other meanings of the word: if you say you understand a mathematical concept or you understand what love is, you are probably not suggesting an ability to make any specific <span id="page_144"></span>
predictions.</div>
<div class="s4P">However, in the discourse of social science, and in most everyday conversations, a claim to understand something is a claim to understand what <span class="s2WT-0">causes</span>
 that thing. The sociologists who collected and studied the thousands of variables in the Fragile Families study were looking for the causes of the outcomes they observed. Physicians who understand what ails a patient are claiming that the pathology they have diagnosed is the cause of the symptoms they have observed. To understand is to describe a causal chain. The ability to make a prediction is a measure of whether such a causal chain has indeed been identified. And correlation, the measure of predictive accuracy, is a measure of how much causation we can explain.</div>
<div class="s4P">This last statement may surprise you if you have been exposed to elementary statistics and remember the often-repeated warning that “correlation does not imply causation.” Consider, for instance, the correlation between shoe size and mathematical ability in children: obviously, one variable does not cause the other. The correlation arises from the fact that both shoe size and math knowledge increase with a child’s age. The correlation is real and supports a prediction: if you know that a child has large feet, you should predict a higher math level than you would if you know that the child has small feet. But you should not infer a causal link from this correlation.</div>
<div class="s4P">We must, however, remember that while correlation does not imply causation, causation <span class="s2WT-0">does</span>
 imply correlation. Where there is a causal link, we should find a correlation. If you find no correlation between age and shoe size among adults, then you can safely conclude that after the end of adolescence, age does not make feet grow larger and that you have to look elsewhere for the causes of differences in shoe size.</div>
<div class="s4P">In short, wherever there is causality, there is correlation. It follows that where there is causality, we should be able to predict—and correlation, the accuracy of this prediction, is a measure of how much causality we understand. Hence the conclusion of the Princeton researchers is this: the extent to which sociologists can predict events like evictions, as measured by a correlation of .22, is an indication of how much—or how little—they understand about the life trajectories of these families. Objective ignorance sets a ceiling <span id="page_145"></span>
not only on our predictions but also on our understanding.</div>
<div class="s4P">What, then, do most professionals mean when they confidently claim to understand their field? How can they make pronouncements about what causes the phenomena they are observing and offer confident predictions about them? In short, why do professionals—and why do we all—seem to underestimate our objective ignorance of the world?</div>
<div class="s7D">Causal Thinking</div>
<div class="s4M">If, as you read the first part of this chapter, you asked yourself what drives evictions and other life outcomes among fragile families, you engaged in the same sort of thinking as that of the researchers whose efforts we described. You applied <span class="s2WT-0">statistical thinking:</span>
 you were concerned with ensembles, such as the population of fragile families, and with the statistics that describe them, including averages, variances, correlations, and so on. You were not focused on individual cases.</div>
<div class="s4P">A different mode of thinking, which comes more naturally to our minds, will be called here <span class="s2WT-0">causal thinking.</span>
 Causal thinking creates stories in which specific events, people, and objects affect one another. To experience causal thinking, picture yourself as a social worker who follows the cases of many underprivileged families. You have just heard that one of these families, the Joneses, has been evicted. Your reaction to this event is informed by what you know about the Joneses. As it happens, Jessica Jones, the family’s breadwinner, was laid off a few months ago. She could not find another job, and since then, she has been unable to pay the rent in full. She made partial payments, pleaded with the building manager several times, and even asked you to intervene (you did, but he remained unmoved). Given this context, the Joneses’ eviction is sad but not surprising. It feels, in fact, like the logical end of a chain of events, the inevitable denouement of a foreordained tragedy.</div>
<div class="s4P">When we give in to this feeling of inevitability, we lose sight of how easily things could have been different—how, at each fork in the road, fate could have taken a different path. Jessica could have kept her job. She could have quickly found another one. A relative <span id="page_146"></span>
could have come to her aid. You, the social worker, could have been a more effective advocate. The building manager could have been more understanding and allowed the family a few weeks of respite, making it possible for Jessica to find a job and catch up with the rent.</div>
<div class="s4P">These alternate narratives are as unsurprising as the main one—if the end is known. Whatever the outcome (eviction or not), once it has happened, causal thinking makes it feel entirely explainable, indeed predictable.</div>
<div class="s7D">Understanding in the Valley of the Normal</div>
<div class="s4M">There is a psychological explanation for this observation. Some events are surprising: a deadly pandemic, an attack on the Twin Towers, a star hedge fund that turns out to be a Ponzi scheme. In our personal lives as well, there are occasional shocks: falling in love with a stranger, the sudden death of a young sibling, an unexpected inheritance. Other events are actively expected, like a second-grader’s return from school at the appointed time.</div>
<div class="s4P">But most human experience falls between these two extremes. We are sometimes in a state in which we actively expect a specific event, and we are sometimes surprised. But most things take place in the broad valley of the normal, where events are neither entirely expected nor especially surprising. At this moment, for example, you have no specific expectation of what is coming in the next paragraph. You would be surprised to find we suddenly switched to Turkish, but there is a wide range of things we could say without shocking you.</div>
<div class="s4P">In the valley of the normal, events unfold just like the Joneses’ eviction: they appear normal in hindsight, although they were not expected, and although we could not have predicted them. This is because the process of understanding reality is backward-looking. An occurrence that was not actively anticipated (the eviction of the Jones family) triggers a search of memory for a candidate cause (the tough job market, the inflexible manager). The search stops when a good narrative is found. Given the opposite outcome, the search would have produced equally compelling causes (Jessica Jones’s tenacity, the understanding manager).</div>
<div class="s4P">As these examples illustrate, many events in a normal story <span id="page_147"></span>
are literally self-explanatory. You may have noted that the building manager in the two versions of the eviction story was not really the same person: the first one was unsympathetic, the second was kind. But your only clue to the manager’s character was the behavior that his character exhibits. Given what we now know about him, his behavior appears coherent. It is the occurrence of the event that tells you its cause.</div>
<div class="s4P">When you explain an unexpected but unsurprising outcome in this way, the destination that is eventually reached always makes sense. This is what we mean by <span class="s2WT-0">understanding</span>
 a story, and this is what makes reality appear predictable—in hindsight. Because the event explains itself as it occurs, we are under the illusion that it could have been anticipated.</div>
<div class="s4P">More broadly, our sense of understanding the world depends on our extraordinary ability to construct narratives that explain the events we observe. The search for causes is almost always successful because causes can be drawn from an unlimited reservoir of facts and beliefs about the world. As anyone who listens to the evening news knows, for example, few large movements of the stock market remain unexplained. The same news flow can “explain” either a fall of the indices (nervous investors are worried about the news!) or a rise (sanguine investors remain optimistic!).</div>
<div class="s4P">When the search for an obvious cause fails, our first resort is to produce an explanation by filling a blank in our model of the world. This is how we infer a fact we had not known before (for instance, that the manager was an unusually kind person). Only when our model of the world cannot be tweaked to generate the outcome do we tag this outcome as surprising and start to search for a more elaborate account of it. Genuine surprise occurs only when routine hindsight fails.</div>
<div class="s4P">This continuous causal interpretation of reality is how we “understand” the world. Our sense of understanding life as it unfolds consists of the steady flow of hindsight in the valley of the normal. This sense is fundamentally causal: new events, once known, eliminate alternatives, and the narrative leaves little room for uncertainty. As we know from classic research on hindsight, even when subjective uncertainty does exist for a while, memories of it are largely erased when the uncertainty is resolved.</div>
<div id="page_148" class="s7D">Inside and Outside</div>
<div class="s4M">We have contrasted two ways of thinking about events: statistical and causal. The causal mode saves us much effortful thinking by categorizing events in real time as normal or abnormal. Abnormal events quickly mobilize costly effort in a search for relevant information, both in the environment and in memory. Active expectation—attentively waiting for something to happen—also demands effort. In contrast, the flow of events in the valley of the normal requires little mental work. Your neighbor may smile as your paths cross or may appear preoccupied and just nod—neither of these events will attract much attention if both have been reasonably frequent in the past. If the smile is unusually wide or the nod unusually perfunctory, you may well find yourself searching your memory for a possible cause. Causal thinking avoids unnecessary effort while retaining the vigilance needed to detect abnormal events.</div>
<div class="s4P">In contrast, statistical thinking is effortful. It requires the attention resources that only System 2, the mode of thinking associated with slow, deliberate thought, can bring to bear. Beyond an elementary level, statistical thinking also demands specialized training. This type of thinking begins with ensembles and considers individual cases as instances of broader categories. The eviction of the Joneses is not seen as resulting from a chain of specific events but is viewed as a statistically likely (or unlikely) outcome, given prior observations of cases that share predictive characteristics with the Joneses.</div>
<div class="s4P">The distinction between these two views is a recurring theme of this book. Relying on causal thinking about a single case is a source of predictable errors. Taking the statistical view, which we will also call the <span class="s2WT-0">outside view</span>
, is a way to avoid these errors.</div>
<div class="s4P">At this point, all we need to emphasize is that the causal mode comes much more naturally to us. Even explanations that should properly be treated as statistical are easily turned into causal narratives. Consider assertions such as “they failed because they lacked experience” or “they succeeded because they had a brilliant leader.” It would be easy for you to think of counterexamples, in which inexperienced teams succeeded and brilliant leaders failed. The correlations of experience and brilliance with success are at best <span id="page_149"></span>
moderate and probably low. Yet a causal attribution is readily made. Where causality is plausible, our mind easily turns a correlation, however low, into a causal and explanatory force. Brilliant leadership is accepted as a satisfactory explanation of success, and inexperience as an explanation of failure.</div>
<div class="s4P">The reliance on flawed explanations is perhaps inevitable, if the alternative is to give up on understanding our world. However, causal thinking and the illusion of understanding the past contribute to overconfident predictions of the future. As we will see, the preference for causal thinking also contributes to the neglect of noise as a source of error, because noise is a fundamentally statistical notion.</div>
<div class="s4P">Causal thinking helps us make sense of a world that is far less predictable than we think. It also explains why we view the world as far more predictable than it really is. In the valley of the normal, there are no surprises and no inconsistencies. The future seems as predictable as the past. And noise is neither heard nor seen.</div>
<div class="s84">Speaking of the Limits of Understanding</div>
<div class="s86">
<span class="class-2">“Correlations of about .20 (PC = 56%) are quite common in human affairs.”</span>
</div>
<div class="s88">
<span class="class-2">“Correlation does not imply causation, but causation does imply correlation.”</span>
</div>
<div class="s88">
<span class="class-2">“Most normal events are neither expected nor surprising, and they require no explanation.”</span>
</div>
<div class="s88">
<span class="class-2">“In the valley of the normal, events are neither expected nor surprising—they just explain themselves.”</span>
</div>
<div class="s8A">
<span class="class-2">“We think we understand what is going on here, but could we have predicted it?”</span>
</div>
</body>
</html>
