<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0031</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_228" class="s6B">
<a href="part0003.xhtml#a2X8">CHAPTER 20</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2X8">Sequencing Information in Forensic Science</a>
</div>
<div class="s4M">
<span class="s2WY">I</span>
n March 2004, a series of bombs placed in commuter trains killed 192 people and injured more than 2,000 in Madrid. A fingerprint found on a plastic bag at the crime scene was transmitted via Interpol to law enforcement agencies worldwide. Days later, the US Federal Bureau of Investigation (FBI) crime lab conclusively identified the fingerprint as belonging to Brandon Mayfield, an American citizen living in Oregon.</div>
<div class="s4P">Mayfield looked like a plausible suspect. A former officer in the US Army, he had married an Egyptian woman and converted to Islam. As a lawyer, he had represented men charged with (and later convicted of) attempting to travel to Afghanistan to join the Taliban. He was on the FBI’s watch list.</div>
<div class="s4P">Mayfield was placed under surveillance, his house bugged and searched, his phones wiretapped. When this scrutiny failed to yield any material information, the FBI arrested him. But he was never formally charged. Mayfield had not left the country in a decade. While he was in custody, the Spanish investigators, who had already informed the FBI that they considered Mayfield a negative match for the fingerprint on the plastic bag, matched that print to another suspect.</div>
<div class="s4P">Mayfield was released after two weeks. Eventually, the US government apologized to him, paid him a $2 million settlement, and ordered an extensive investigation into the causes of the mistake. Its key finding: “The error was a human error and not a methodology or technology failure.”</div>
<div class="s4P">Fortunately, such human errors are rare. They are <span id="page_229"></span>
nonetheless instructive. How could the best fingerprint experts in the United States mistakenly identify a fingerprint as belonging to a man who had never come close to the crime scene? To find out, we first need to understand how fingerprint examination works and how it relates to other examples of professional judgment. We will learn that forensic fingerprinting, which we tend to think of as an exact science, is in fact subject to the psychological biases of examiners. These biases can create more noise, and thus more error, than we would imagine. And we will see how the forensic science community is taking steps to tackle this problem by implementing a decision hygiene strategy that can apply to all environments: a tight control over the flow of information used to make judgments.</div>
<div class="s7D">Fingerprints</div>
<div class="s4M">Fingermarks are the impressions left by the friction ridges of our fingers on the surfaces we touch. Although there are examples of fingerprints being used as apparent identification marks in ancient times, modern fingerprinting dates back to the late nineteenth century, when Henry Faulds, a Scottish physician, published the first scientific paper suggesting the use of fingerprints as an identification technique.</div>
<div class="s4P">In subsequent decades, fingerprints gained traction as identification marks in criminal records, gradually replacing the anthropometric measurement techniques developed by Alphonse Bertillon, a French police officer. Bertillon himself codified, in 1912, a formal system for the comparison of fingerprints. Sir Francis Galton, whom we previously encountered as the discoverer of the wisdom of crowds, had developed a similar system in England. (Still, it is no wonder that these founding fathers are rarely celebrated. Galton believed that fingerprints would be a useful tool for classifying individuals according to their race, and Bertillon, probably because of anti-Semitic prejudice, contributed decisive—and flawed—expert testimony during the 1894 and 1899 trials of Alfred Dreyfus.)</div>
<div class="s4P">Police officers soon discovered that fingerprints could do more than serve as identification marks for repeat offenders. In 1892, Juan Vucetich, a police officer in Argentina, was the first to <span id="page_230"></span>
compare a latent fingerprint left at a crime scene with a suspect’s thumb. Since then, the practice of collecting <span class="s2WT-0">latent prints</span>
 (those left by their owner at the scene of a crime) and comparing them with <span class="s2WT-0">exemplar prints</span>
 (those collected in controlled conditions from known individuals) has been the most decisive application of fingerprinting and has provided the most widely used form of forensic evidence.</div>
<div class="s4P">If you have ever come across an electronic fingerprint reader (like those used by immigration services in many countries), you probably think of fingerprint comparison as a straightforward, mechanical, and easily automated task. But comparing a latent print collected from a crime scene with an exemplar print is a much more delicate exercise than matching two clean prints. When you press your fingers firmly on a reader purposely built to record a fingerprint impression, you produce a neat, standardized image. By contrast, latent prints are often partial, unclear, smudged, or otherwise distorted; they do not provide the same quantity and quality of information as does a print collected in a controlled and dedicated environment. Latent prints often overlap with other prints, either by the same person or by someone else, and include dirt and other artifacts present on the surface. Deciding whether they match a suspect’s exemplar prints requires expert judgment. It is the job of human fingerprint examiners.</div>
<div class="s4P">When provided with a latent print, examiners routinely follow a process called ACE-V, which stands for analysis, comparison, evaluation, and verification. First, they must analyze the latent print to determine whether it is of sufficient value for comparison. If it is, they compare it to an exemplar print. The comparison leads to an evaluation, which can produce an <span class="s2WT-0">identification</span>
 (the prints originated from the same person), an <span class="s2WT-0">exclusion</span>
 (the prints do not originate from the same person), or an inconclusive decision. An identification decision triggers the fourth step: verification by another examiner.</div>
<div class="s4P">For decades, the reliability of this procedure remained unquestioned. Although eyewitness testimonies have been shown to be dangerously unreliable and even confessions can be false, fingerprints were accepted—at least until the advent of DNA analysis—as the most credible form of evidence. Until 2002, fingerprint evidence had never been successfully challenged in an American <span id="page_231"></span>
courtroom. The FBI website at the time, for example, was adamant: “Fingerprints offer an <span class="s2WT-0">infallible</span>
 means of personal identification.” In the very rare cases when errors did happen, they were blamed on incompetence or fraud.</div>
<div class="s4P">Fingerprint evidence remained unchallenged for so long in part because of the difficulty in proving it wrong. The true value of a set of fingerprints, that is, the ground truth of who actually committed the crime, is often unknown. For Mayfield and a handful of similar cases, the mistake was especially egregious. But in general, if a suspect disputes the examiner’s conclusions, the fingerprint evidence will, of course, be considered more reliable.</div>
<div class="s4P">We have noted that not knowing the true value is neither unusual nor an impediment to measuring noise. How much noise is there in fingerprint analysis? Or more precisely, given that fingerprint examiners, unlike sentencing judges or underwriters, do not produce a number but make a categorical judgment, how often do they disagree, and why? This question is what Itiel Dror, a cognitive neuroscience researcher at University College London, was the first to set out to study. He conducted what amounts to a series of noise audits in a field that had assumed it did not have a noise problem.</div>
<div class="s7D">Occasion Noise in Fingerprint Analysis</div>
<div class="s4M">It may seem odd for a cognitive scientist—a psychologist—to challenge fingerprint examiners. After all, as you may have seen on TV shows like <span class="s2WT-0">CSI: Crime Scene Investigation</span>
 and subsequent series of the CSI franchise, these are latex-glove-wearing, microscope-wielding hard-science types. But Dror realized that examining fingerprints was clearly a matter of judgment. And as a cognitive neuroscientist, he reasoned that wherever there is judgment, there must be noise.</div>
<div class="s4P">To test this hypothesis, Dror focused first on occasion noise: the variability between the judgments of <span class="s2WT-0">the same</span>
 experts looking at <span class="s2WT-0">the same</span>
 evidence twice. As Dror puts it, “If experts are not reliable in the sense that they are not consistent with themselves, then the basis of their judgments and professionalism is in question.”</div>
<div class="s4P">Fingerprints provide a perfect test bed for an audit of <span id="page_232"></span>
occasion noise because unlike the cases that a physician or a judge encounters, pairs of prints are not easily memorable. Of course, a suitable interval of time must be allowed to pass to ensure that examiners do not remember the prints. (In Dror’s studies, some brave, open-minded experts agreed that, <span class="s2WT-0">at any time in the next five years,</span>
 they would take part in studies, without their knowledge.) Additionally, the experiment must happen in the course of the experts’ routine casework, so that they are not aware that their skills are being tested. If, under these circumstances, the examiners’ judgments change from one test to the next, we are in the presence of occasion noise.</div>
<div class="s7D">The Forensic Confirmation Bias</div>
<div class="s4M">In two of his original studies, Dror added an important twist. When seeing the prints for the second time, some of the examiners were exposed to additional biasing information about the case. For instance, fingerprint examiners who had earlier found the prints to be a match were told, this time, that “the suspect has an alibi” or that “firearms evidence suggests it’s not him.” Others, who had first concluded that a suspect was innocent or that the prints were inconclusive, were told the second time that “the detective believes the suspect is guilty,” “eyewitnesses identified him,” or “he confessed to the crime.” Dror called this experiment a test of the experts’ “biasability,” because the contextual information supplied activated a psychological bias (a confirmation bias) in a given direction.</div>
<div class="s4P">Indeed, the examiners turned out to be susceptible to bias. When the same examiners considered the same prints they had seen earlier, but this time with biasing information, their judgments changed. In the first study, four out of five experts altered their previous identification decision when presented with strong contextual information that suggested an exclusion. In the second study, six experts reviewed four pairs of prints; biasing information led to changes in four of the twenty-four decisions. To be sure, most of their decisions did not change, but for these kinds of decisions, a shift of one in six can be counted as large. These findings have since been replicated by other researchers.</div>
<div id="page_233" class="s4P">Predictably, the examiners were more likely to change their minds when the decision was a difficult one to start with, when the biasing information was strong, and when the change was from a conclusive to an inconclusive decision. It is, nonetheless, troubling that “expert fingerprint examiners made decisions on the basis of the context, rather than on the basis of the actual information contained in the print.”</div>
<div class="s4P">The effect of biasing information is not restricted to the examiner’s conclusion (identification, inconclusive, or exclusion). Biasing information actually changes <span class="s2WT-0">what</span>
 the examiner perceives, in addition to <span class="s2WT-0">how</span>
 that perception is interpreted. In a separate study, Dror and colleagues showed that examiners who have been placed in a biased context literally do not see the same things as those who have not been exposed to biasing information. When the latent print is accompanied by a target exemplar print, the examiners observe significantly fewer details (called <span class="s2WT-0">minutiae</span>
) than they do when they see the latent print alone. A later, independent study confirmed this conclusion and added that “how [it] occurs is not obvious.”</div>
<div class="s4P">Dror coined a term for the impact of biasing information: the <span class="s2WT-0">forensic confirmation bias.</span>
 This bias has since been documented with other forensic techniques, including blood pattern analysis, arson investigation, the analysis of skeletal remains, and forensic pathology. Even DNA analysis—widely regarded as the new gold standard in forensic science—can be susceptible to confirmation bias, at least when experts must assess complex DNA mixtures.</div>
<div class="s4P">The susceptibility of forensic experts to confirmation bias is not just a theoretical concern because, in reality, no systematic precautions are in place to make sure that forensic experts are not exposed to biasing information. Examiners often receive such information in the transmittal letters that accompany the evidence submitted to them. Examiners are also often in direct communication with police, prosecutors, and other examiners.</div>
<div class="s4P">Confirmation bias raises another problem. An important safeguard against errors, built into the ACE-V procedure, is the independent verification by another expert before an identification can be confirmed. But most often, only identifications are independently verified. The result is a strong risk of confirmation bias, as the verifying examiner knows that the initial conclusion was <span id="page_234"></span>
an identification. The verification step therefore does not provide the benefit normally expected from the aggregation of independent judgments, because verifications are not, in fact, independent.</div>
<div class="s4P">A cascade of confirmation biases seems to have been at work in the Mayfield case, in which not two but three FBI experts concurred on the erroneous identification. As the later investigation of the error noted, the first examiner appears to have been impressed by “the power of the correlation” from the automated system searching the databases of fingerprints for a possible match. Although he was, apparently, not exposed to Mayfield’s biographical details, the results provided by the computerized system performing the initial search, “coupled with the inherent pressure of working an extremely high-profile case,” were enough to produce the initial confirmation bias. Once the first examiner made an erroneous identification, the report continues, “the subsequent examinations were tainted.” As the first examiner was a highly respected supervisor, “it became increasingly difficult for others in the agency to disagree.” The initial error was replicated and amplified, resulting in a near-certainty that Mayfield was guilty. Tellingly, even a highly respected independent expert, appointed by the court to examine the evidence on behalf of Mayfield’s defense, concurred with the FBI in confirming the identification.</div>
<div class="s4P">The same phenomenon can be at work in other forensic disciplines and across them. Latent print identification is reputed to be among the most objective of the forensic disciplines. If fingerprint examiners can be biased, so can experts in other fields. Moreover, if a firearms expert knows that the fingerprints are a match, this knowledge may bias that expert’s judgment, too. And if a forensic odontologist knows that DNA analysis has identified a suspect, that expert is probably less likely to suggest that the bite marks do not match the suspect. These examples raise the specter of bias cascades: just as in the group decisions we described in <a href="part0016.xhtml">chapter 8</a>
, an initial error prompted by confirmation bias becomes the biasing information that influences a second expert, whose judgment biases a third one, and so on.</div>
<div class="s4P">Having established that biasing information creates variability, Dror and his colleagues uncovered more evidence of occasion noise. Even when fingerprint experts are not exposed to biasing information, they sometimes change their minds about a set <span id="page_235"></span>
of prints they have seen before. As we would expect, changes are less frequent when no biasing information is supplied, but they happen nonetheless. A 2012 study commissioned by the FBI replicated this finding on a larger scale by asking seventy-two examiners to look again at twenty-five pairs of prints they had evaluated about seven months earlier. With a large sample of highly qualified examiners, the study confirmed that fingerprint experts are sometimes susceptible to occasion noise. About one decision in ten was altered. Most of the changes were to or from the inconclusive category, and none resulted in false identifications. The study’s most troubling implication is that some fingerprint identifications that led to convictions could potentially have been judged inconclusive at another time. When the same examiners are looking at the same prints, even when the context is not designed to bias them but is instead meant to be as constant as possible, there is inconsistency in their decisions.</div>
<div class="s7D">Some Noise, but How Much Error?</div>
<div class="s4M">The practical question raised by these findings is the possibility of judicial errors. We cannot ignore questions about the reliability of experts who testify in court: validity requires reliability because, quite simply, it is hard to agree with reality if you cannot agree with yourself.</div>
<div class="s4P">How many errors, exactly, are caused by faulty forensic science? A review of 350 exonerations obtained by the Innocence Project, a nonprofit that works to overturn wrongful convictions, concluded that the misapplication of forensic science was a contributing cause in 45% of cases. This statistic sounds bad, but the question that matters to judges and jurors is different: To know how much trust they should accord the examiner taking the stand to testify, they need to know how likely forensic scientists, including fingerprint examiners, are to make consequential errors.</div>
<div class="s4P">The most robust set of answers to this question can be found in a report by the President’s Council of Advisors on Science and Technology (PCAST), an advisory group of the nation’s leading scientists and engineers, which in 2016 produced an in-depth review of forensic science in criminal courts. The report summarizes the <span id="page_236"></span>
available evidence on the validity of fingerprint analysis and especially on the likelihood of erroneous identifications (false positives) such as the one involving Mayfield.</div>
<div class="s4P">This evidence is surprisingly sparse, and as PCAST notes, it is “distressing” that work to produce it did not begin until recently. The most credible data come from the only published large-scale study of fingerprint identification accuracy, which was conducted by FBI scientists themselves in 2011. The study involved 169 examiners, each comparing approximately one hundred pairs of latent and exemplar fingerprints. Its central finding was that very few erroneous identifications occurred: the false-positive rate was about one in six hundred.</div>
<div class="s4P">An error rate of one in six hundred is low but, as the report noted, is “<span class="s2WT-0">much higher</span>
 than the general public (and, by extension, most jurors) would likely believe based on longstanding claims about the accuracy of fingerprint analysis.” Furthermore, this study contained no biasing contextual information, and the participating examiners knew they were taking part in a test—which may have caused the study to underestimate the errors that occur in real casework. A subsequent study conducted in Florida arrived at much higher numbers of false positives. The varied findings in the literature suggest that we need more research on the accuracy of fingerprint examiner decisions and how these decisions are reached.</div>
<div class="s4P">One reassuring finding that does seem consistent across all studies, however, is that the examiners appear to err on the side of caution. Their accuracy is not perfect, but they are aware of the consequences of their judgments, and they take into account the asymmetrical cost of possible errors. Because of the very high credibility of fingerprinting, an erroneous identification can have tragic effects. Other types of error are less consequential. For instance, FBI experts observe, “in most casework, an exclusion has the same operational implications as an inconclusive.” In other words, the fact that a fingerprint is found on the murder weapon is sufficient to convict, but the absence of that print is not sufficient to exonerate a suspect.</div>
<div class="s4P">Consistent with our observation of examiner caution, the evidence suggests that experts think twice—or much more than twice—before making an identification decision. In the FBI study of identification accuracy, less than one-third of “mated” pairs (where <span id="page_237"></span>
the latent and the exemplar are from the same person) were judged (accurately) as identifications. Examiners also make far fewer false-positive identifications than false-negative exclusions. They are susceptible to bias, but not equally in both directions. As Dror notes, “It is easier to bias forensic experts towards the non-committal conclusion of ‘inconclusive’ than to the definitive ‘identification’ conclusion.”</div>
<div class="s4P">Examiners are trained to consider erroneous identification as the deadly sin to be avoided at all costs. To their credit, they act in accordance with this principle. We can only hope that their level of care keeps erroneous identifications, like those in the Mayfield case and a handful of other high-profile cases, extremely rare.</div>
<div class="s7D">Listening to Noise</div>
<div class="s4M">To observe that there is noise in forensic science should not be seen as a criticism of forensic scientists. It is merely a consequence of the observation we have made repeatedly: Wherever there is judgment, there is noise, and more of it than you think. A task like the analysis of fingerprints seems objective, so much so that many of us would not spontaneously regard it as a form of judgment. Yet it leaves room for inconsistency, disagreement, and, occasionally, error. However low the error rate of fingerprint identification may be, it is not zero, and as PCAST noted, juries should be made aware of that.</div>
<div class="s4P">The first step to reduce noise must be, of course, to acknowledge its possibility. This admission does not come naturally to members of the fingerprint community, many of whom were initially highly skeptical of Dror’s noise audit. The notion that an examiner can be unwittingly influenced by information about the case irked many experts. In a reply to Dror’s study, the chair of the Fingerprint Society wrote that “any fingerprint examiner who… is swayed either way in that decision making process… is so immature he/she should seek employment in Disneyland.” A director of a major forensic laboratory noted that having access to case information—precisely the sort of information that could bias the examiner—“provides some personal satisfaction which allows [examiners] to enjoy their job <span class="s2WT-0">without actually altering their judgment.</span>
” Even the FBI, in its internal investigation of the Mayfield case, noted that “latent print examiners routinely <span id="page_238"></span>
conduct verifications in which they know the previous examiners’ results <span class="s2WT-0">and yet those results do not influence the examiner’s conclusions.</span>
” These remarks essentially amount to a denial of the existence of confirmation bias.</div>
<div class="s4P">Even when they are aware of the risk of bias, forensic scientists are not immune to the bias blind spot: the tendency to acknowledge the presence of bias in others, but not in oneself. In a survey of four hundred professional forensic scientists in twenty-one countries, 71% agreed that “cognitive bias is a cause for concern in the forensic sciences as a whole,” but only 26% thought that their “own judgments are influenced by cognitive bias.” In other words, about half of these forensic professionals believe that their colleagues’ judgments are noisy but that their own are not. Noise can be an invisible problem, even to people whose job is to see the invisible.</div>
<div class="s7D">Sequencing Information</div>
<div class="s4M">Thanks to the persistence of Dror and his colleagues, attitudes are slowly changing and a growing number of forensic laboratories have begun taking new measures to reduce error in their analyses. For example, the PCAST report commended the FBI laboratory for redesigning its procedures to minimize the risk of confirmation bias.</div>
<div class="s4P">The necessary methodological steps are relatively simple. They illustrate a decision hygiene strategy that has applicability in many domains: <span class="s2WT-0">sequencing information to limit the formation of premature intuitions</span>
. In any judgment, some information is relevant, and some is not. More information is not always better, especially if it has the potential to bias judgments by leading the judge to form a premature intuition.</div>
<div class="s4P">In that spirit, the new procedures deployed in forensic laboratories aim to protect the independence of the examiners’ judgments by giving the examiners only the information they need, when they need it. In other words, the laboratory keeps them as much in the dark about the case as possible and reveals information only gradually. To do that, the approach Dror and colleagues codified is called <span class="s2WT-0">linear sequential unmasking.</span>
</div>
<div id="page_239" class="s4P">Dror has another recommendation that illustrates the same decision hygiene strategy: examiners should document their judgments at each step. They should document their analysis of a latent fingerprint <span class="s2WT-0">before</span>
 they look at exemplar fingerprints to decide whether they are a match. This sequence of steps helps experts avoid the risk that they see only what they are looking for. And they should record their judgment on the evidence before they have access to contextual information that risks biasing them. If they change their mind after they are exposed to contextual information, these changes, and the rationale for them, should be documented. This requirement limits the risk that an early intuition biases the entire process.</div>
<div class="s4P">The same logic inspires a third recommendation, which is an important part of decision hygiene. When a different examiner is called on to verify the identification made by the first person, the second person should not be aware of the first judgment.</div>
<div class="s4P">The presence of noise in forensic science is, of course, of concern because of its potential life-or-death consequences. But it is also revealing. That we remained for so long entirely unaware of the possibility of error in fingerprint identification shows how our confidence in expert human judgment can sometimes be exaggerated and how a noise audit can reveal an unexpected amount of noise. The ability to mitigate these shortcomings through relatively simple process changes should be encouraging to all those who care about improving the quality of decisions.</div>
<div class="s4P">The main decision hygiene strategy this case illustrates—sequencing information—has broad applicability as a safeguard against occasion noise. As we have noted, occasion noise is driven by countless triggers, including mood and even outside temperature. You cannot hope to control all these triggers, but you can attempt to shield judgments from the most obvious ones. You already know, for instance, that judgments can be altered by anger, fear, or other emotions, and perhaps you have noted that it is a good practice, if you can, to revisit your judgment at different points in time, when the triggers of occasion noise are likely to be different.</div>
<div class="s4P">Less obvious is the possibility that your judgment can be altered by another trigger of occasion noise: information—even when it is accurate information. As in the example of the fingerprint examiners, as soon as you know what others think, confirmation bias <span id="page_240"></span>
can lead you to form an overall impression too early and to ignore contradictory information. The titles of two Hitchcock movies sum it up: a good decision maker should aim to keep a “shadow of a doubt,” not to be “the man who knew too much.”</div>
<div class="s84">Speaking of Sequencing Information</div>
<div class="s86">
<span class="class-2">“Wherever there is judgment, there is noise—and that includes reading fingerprints.”</span>
</div>
<div class="s88">
<span class="class-2">“We have more information about this case, but let’s not tell the experts everything we know before they make their judgment, so as not to bias them. In fact, let’s tell them only what they absolutely need to know.”</span>
</div>
<div class="s88">
<span class="class-2">“The second opinion is not independent if the person giving it knows what the first opinion was. And the third one, even less so: there can be a bias cascade.”</span>
</div>
<div class="s8A">
<span class="class-2">“To fight noise, they first have to admit that it exists.”</span>
</div>
</body>
</html>
