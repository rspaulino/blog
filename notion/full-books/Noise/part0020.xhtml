<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0020</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_130" class="s6B">
<a href="part0003.xhtml#a2Y0">CHAPTER 11</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2Y0">Objective Ignorance</a>
</div>
<div class="s4M">
<span class="s2WY">W</span>
e have often had the experience of sharing with audiences of executives the material of the last two chapters, with its sobering findings about the limited achievements of human judgment. The message we aim to convey has been around for more than half a century, and we suspect that few decision makers have avoided exposure to it. But they are certainly able to resist it.</div>
<div class="s4P">Some of the executives in our audiences tell us proudly that they trust their gut more than any amount of analysis. Many others are less blunt but share the same view. Research in managerial decision making has shown that executives, especially the more senior and experienced ones, resort extensively to something variously called <span class="s2WT-0">intuition, gut feel,</span>
 or, simply, <span class="s2WT-0">judgment</span>
 (used in a different sense from the one we use in this book).</div>
<div class="s4P">In short, decision makers like to listen to their gut, and most seem happy with what they hear. Which raises a question: what, exactly, do these people, who are blessed with the combination of authority and great self-confidence, hear from their gut?</div>
<div class="s4P">One review of intuition in managerial decision making defines it as “a judgment for a given course of action that comes to mind with an aura or conviction of rightness or plausibility, but without clearly articulated reasons or justifications—essentially ‘knowing’ but without knowing why.” We propose that this sense of knowing without knowing why is actually the <span class="s2WT-0">internal signal</span>
 of judgment completion that we mentioned in <a href="part0012.xhtml">chapter 4</a>
.</div>
<div class="s4P">The internal signal is a self-administered reward, one people work hard (or sometimes not so hard) to achieve when they reach closure on a judgment. It is a satisfying emotional experience, a pleasing sense of coherence, in which the evidence considered and <span id="page_131"></span>
the judgment reached feel right. All the pieces of the jigsaw puzzle seem to fit. (We will see later that this sense of coherence is often bolstered by hiding or ignoring pieces of evidence that don’t fit.)</div>
<div class="s4P">What makes the internal signal important—and misleading—is that it is construed not as a feeling but as a belief. This emotional experience (“the evidence feels right”) masquerades as rational confidence in the validity of one’s judgment (“I know, even if I don’t know why”).</div>
<div class="s4P">Confidence is no guarantee of accuracy, however, and many confident predictions turn out to be wrong. While both bias and noise contribute to prediction errors, the largest source of such errors is not the limit on how good predictive judgments <span class="s2WT-0">are.</span>
 It is the limit on how good they <span class="s2WT-0">could be.</span>
 This limit, which we call <span class="s2WT-0">objective ignorance,</span>
 is the focus of this chapter.</div>
<div class="s7D">Objective Ignorance</div>
<div class="s4M">Here is a question you can ask yourself if you find yourself making repeated predictive judgments. The question could apply to any task—picking stocks, for instance, or predicting the performance of professional athletes. But for simplicity, we’ll choose the same example we used in <a href="part0018.xhtml">chapter 9</a>
: the selection of job candidates. Imagine you have evaluated a hundred candidates over the years. You now have a chance to assess how good your decisions were, by comparing the evaluations you had made with the candidates’ objectively assessed performance since then. If you pick a pair of candidates at random, how often would your ex ante judgment and the ex post evaluations agree? In other words, when comparing any two candidates, what is the probability that the one you thought had more potential did in fact turn out to be the higher performer?</div>
<div class="s4P">We often informally poll groups of executives on this question. The most frequent answers are in the 75–85% range, and we suspect that these responses are constrained by modesty and by a wish not to appear boastful. Private, one-on-one conversations suggest that the true sense of confidence is often even higher.</div>
<div class="s4P">Since you are now familiar with the percent concordant statistic, you can easily see the problem this evaluation raises. A PC of 80% roughly corresponds to a correlation of .80. This level of <span id="page_132"></span>
predictive power is rarely achieved in the real world. In the field of personnel selection, a recent review found that the performance of human judges does not come close to this number. On average, they achieve a predictive correlation of .28 (PC = 59%).</div>
<div class="s4P">If you consider the challenge of personnel selection, the disappointing results are not that surprising. A person who starts a new job today will encounter many challenges and opportunities, and chance will intervene to change the direction of her life in many ways. She may encounter a supervisor who believes in her, creates opportunities, promotes her work, and builds her self-confidence and motivation. She may also be less lucky and, through no fault of her own, start her career with a demoralizing failure. In her personal life, too, there may be events that affect her job performance. None of these events and circumstances can be predicted today—not by you, not by anyone else, and not by the best predictive model in the world. This intractable uncertainty includes everything that cannot be known at this time about the outcome that you are trying to predict.</div>
<div class="s4P">Furthermore, much about the candidates is in principle knowable but is not known when you make your judgment. For our purposes, it does not matter whether these gaps in knowledge come from the lack of sufficiently predictive tests, from your decision that the cost of acquiring more information was not justified, or from your own negligence in fact-finding. One way or the other, you are in a state of less-than-perfect information.</div>
<div class="s4P">Both intractable uncertainty (what cannot possibly be known) and imperfect information (what could be known but isn’t) make perfect prediction impossible. These unknowns are not problems of bias or noise in your judgment; they are objective characteristics of the task. This objective ignorance of important unknowns severely limits achievable accuracy. We take a terminological liberty here, replacing the commonly used <span class="s2WT-0">uncertainty</span>
 with <span class="s2WT-0">ignorance.</span>
 This term helps limit the risk of confusion between uncertainty, which is about the world and the future, and noise, which is variability in judgments that should be identical.</div>
<div class="s4P">There is more information (and less objective ignorance) in some situations than in others. Most professional judgments are pretty good. With respect to many illnesses, doctors’ predictions are <span id="page_133"></span>
excellent, and for many legal disputes, lawyers can tell you, with great accuracy, how judges are likely to rule.</div>
<div class="s4P">In general, however, you can safely expect that people who engage in predictive tasks will underestimate their objective ignorance. Overconfidence is one of the best-documented cognitive biases. In particular, judgments of one’s ability to make precise predictions, even from limited information, are notoriously overconfident. What we said of noise in predictive judgments can also be said of objective ignorance: wherever there is prediction, there is ignorance, and more of it than you think.</div>
<div class="s7D">Overconfident Pundits</div>
<div class="s4M">A good friend of ours, the psychologist Philip Tetlock, is armed with a fierce commitment to truth and a mischievous sense of humor. In 2005, he published a book titled <span class="s2WT-0">Expert Political Judgment.</span>
 Despite that neutral-sounding title, the book amounted to a devastating attack on the ability of experts to make accurate predictions about political events.</div>
<div class="s4P">Tetlock studied the predictions of almost three hundred experts: prominent journalists, respected academics, and high-level advisers to national leaders. He asked whether their political, economic, and social forecasts came true. The research spanned two decades; to find out whether long-term predictions are right, you need patience.</div>
<div class="s4P">Tetlock’s key finding was that in their predictions about major political events, the supposed experts are stunningly unimpressive. The book became famous for its arresting punch line: “The average expert was roughly as accurate as a dart-throwing chimpanzee.” A more precise statement of the book’s message was that experts who make a living “commenting or offering advice on political and economic trends” were not “better than journalists or attentive readers of the <span class="s2WT-0">New York Times</span>
 in ‘reading’ emerging situations.” For sure, the experts told great stories. They could analyze a situation, paint a compelling picture of how it would evolve, and refute, with great confidence, the objections of those who disagreed with them in television studios. But did they actually know what would happen? Hardly.</div>
<div id="page_134" class="s4P">Tetlock reached this conclusion by cutting through the storytelling. For each issue, he asked the experts to assign probabilities to three possible outcomes: status quo, more of something, or less of it. A dart-throwing chimp would “choose” each of these outcomes with the same probability—one-third—regardless of reality. Tetlock’s experts barely exceeded this very low standard. On average, they assigned slightly higher probabilities to events that occurred than to those that did not, but the most salient feature of their performance was their excessive confidence in their predictions. Pundits blessed with clear theories about how the world works were the most confident and the least accurate.</div>
<div class="s4P">Tetlock’s findings suggest that detailed long-term predictions about specific events are simply impossible. The world is a messy place, where minor events can have large consequences. For example, consider the fact that at the instant of conception, there was an even chance that every significant figure in history (and also the insignificant ones) would be born with a different gender. Unforeseeable events are bound to occur, and the consequences of these unforeseeable events are also unforeseeable. As a result, objective ignorance accumulates steadily the further you look into the future. The limit on expert political judgment is set not by the cognitive limitation of forecasters but by their intractable objective ignorance of the future.</div>
<div class="s4P">Our conclusion, then, is that pundits should not be blamed for the failures of their distant predictions. They do, however, deserve some criticism for attempting an impossible task and for believing they can succeed in it.</div>
<div class="s4P">Some years after his shocking discovery of the futility of much long-term forecasting, Tetlock teamed up with his spouse, Barbara Mellers, to study how well people do when asked to forecast world events in the relatively short term—usually less than a year. The team discovered that short-term forecasting is difficult but not impossible, and that some people, whom Tetlock and Mellers called <span class="s2WT-0">superforecasters,</span>
 are consistently better at it than most others, including professionals in the intelligence community. In the terms we use here, their new findings are compatible with the notion that objective ignorance increases as we look further into the future. We return to superforecasters in <a href="part0032.xhtml">chapter 21</a>
.</div>
<div id="page_135" class="s7D">Poor Judges and Barely Better Models</div>
<div class="s4M">Tetlock’s early research demonstrated people’s general inability to do well in long-term political forecasting. Finding even one person with a clear crystal ball would have changed the conclusions completely. A task can be deemed impossible only after many credible actors have tried their hand and failed. As we have shown that mechanical aggregation of information is often superior to human judgment, the predictive accuracy of rules and algorithms provides a better test of how intrinsically predictable, or unpredictable, outcomes are.</div>
<div class="s4P">The previous chapters may have given you the impression that algorithms are crushingly superior to predictive judgments. That impression, however, would be misleading. Models are consistently better than people, but not much better. There is essentially no evidence of situations in which people do very poorly and models do very well with the same information.</div>
<div class="s4P">In <a href="part0018.xhtml">chapter 9</a>
, we mentioned a review of 136 studies that demonstrated the superiority of mechanical aggregation over clinical judgment. While the evidence of that superiority is indeed “massive and consistent,” the performance gap is not large. Ninety-three of the studies in the review focused on binary decisions and measured the “hit rate” of clinicians and formulas. In the median study, clinicians were right 68% of the time, formulas 73% of the time. A smaller subset of 35 studies used the correlation coefficient as a measure of accuracy. In these studies, clinicians achieved a median correlation with the outcome of .32 (PC = 60%), while formulas achieved .56 (PC = 69%). On both metrics, formulas are consistently better than clinicians, but the limited validity of the mechanical predictions remains striking. The performance of models does not change the picture of a fairly low ceiling of predictability.</div>
<div class="s4P">What about artificial intelligence? As we noted, AI often performs better than simpler models do. In most applications, however, its performance remains far from perfect. Consider, for instance, the bail-prediction algorithm we discussed in <a href="part0019.xhtml">chapter 10</a>
. We noted that, keeping constant the number of people who are denied bail, the algorithm could reduce crime rates by up to 24%. This is an impressive improvement on the predictions of human bail judges, but if the algorithm could predict with perfect accuracy <span id="page_136"></span>
which defendants will reoffend, it could reduce the crime rate much more. The supernatural predictions of future crimes in <span class="s2WT-0">Minority Report</span>
 are science fiction for a reason: there is a large amount of objective ignorance in the prediction of human behavior.</div>
<div class="s4P">Another study, led by Sendhil Mullainathan and Ziad Obermeyer, modeled the diagnosis of heart attacks. When patients present signs of a possible heart attack, emergency room physicians must decide whether to prescribe additional tests. In principle, patients should be tested only when the risk of a heart attack is high enough: because testing is not just costly but also invasive and risky, it is undesirable for low-risk patients. Thus a physician’s decision to prescribe tests requires an assessment of heart attack risk. The researchers built an AI model to make this assessment. The model uses more than twenty-four hundred variables and is based on a large sample of cases (4.4 million Medicare visits by 1.6 million patients). With this amount of data, the model probably approaches the limits of objective ignorance.</div>
<div class="s4P">Not surprisingly, the AI model’s accuracy is distinctly superior to that of physicians. To evaluate the performance of the model, consider the patients whom the model placed in the highest decile of risk. When these patients were tested, 30% of them turned out to have had a heart attack, whereas 9.3% of the patients in the middle of the risk distribution had experienced one. This level of discrimination is impressive, but it is also far from perfect. We can reasonably conclude that the performance of the physicians is limited at least as much by the constraints of objective ignorance as by the imperfections of their judgments.</div>
<div class="s7D">The Denial of Ignorance</div>
<div class="s4M">By insisting on the impossibility of perfect prediction, we might seem to be stating the obvious. Admittedly, asserting that the future is unpredictable is hardly a conceptual breakthrough. However, the obviousness of this fact is matched only by the regularity with which it is ignored, as the consistent findings about predictive overconfidence demonstrate.</div>
<div class="s4P">The prevalence of overconfidence sheds new light on our informal poll of gut-trusting decision makers. We have noted that <span id="page_137"></span>
people often mistake their subjective sense of confidence for an indication of predictive validity. After you reviewed the evidence in <a href="part0018.xhtml">chapter 9</a>
 about Nathalie and Monica, for instance, the internal signal you felt when you reached a coherent judgment gave you confidence that Nathalie was the stronger candidate. If you were confident in that prediction, however, you fell for the illusion of validity: the accuracy you can achieve with the information you were given is quite low.</div>
<div class="s4P">People who believe themselves capable of an impossibly high level of predictive accuracy are not just overconfident. They don’t merely deny the risk of noise and bias in their judgments. Nor do they simply deem themselves superior to other mortals. They also believe in the predictability of events that are in fact unpredictable, implicitly denying the reality of uncertainty. In the terms we have used here, this attitude amounts to a <span class="s2WT-0">denial of ignorance</span>
.</div>
<div class="s4P">The denial of ignorance adds an answer to the puzzle that baffled Meehl and his followers: why his message has remained largely unheeded, and why decision makers continue to rely on their intuition. When they listen to their gut, decision makers hear the internal signal and feel the emotional reward it brings. This internal signal that a good judgment has been reached is the voice of confidence, of “knowing without knowing why.” But an objective assessment of the evidence’s true predictive power will rarely justify that level of confidence.</div>
<div class="s4P">Giving up the emotional reward of intuitive certainty is not easy. Tellingly, leaders say they are especially likely to resort to intuitive decision making in situations that they perceive as highly uncertain. When the facts deny them the sense of understanding and confidence they crave, they turn to their intuition to provide it. The denial of ignorance is all the more tempting when ignorance is vast.</div>
<div class="s4P">The denial of ignorance also explains another puzzle. When faced with the evidence we have presented here, many leaders draw a seemingly paradoxical conclusion. Their gut-based decisions may not be perfect, they argue, but if the more systematic alternatives are also far from perfect, they are not worth adopting. Recall, for instance, that the average correlation between the ratings of human judges and employee performance is .28 (PC = 59%). According to the same study, and consistent with the evidence we reviewed, mechanical prediction might do better, but not by much: its <span id="page_138"></span>
predictive accuracy is .44 (PC = 65%). An executive might ask: why bother?</div>
<div class="s4P">The answer is that in something as important as decisions about whom to hire, this increase in validity has a great deal of value. The same executives routinely make significant changes in their ways of working to capture gains that are not nearly as large. Rationally, they understand that success can never be guaranteed and that a higher chance of success is what they are striving for in their decisions. They also understand probability. None of them would buy a lottery ticket that had a 59% chance of winning if they could buy, for the same price, one with a 65% chance.</div>
<div class="s4P">The challenge is that the “price” in this situation is not the same. Intuitive judgment comes with its reward, the internal signal. People are prepared to trust an algorithm that achieves a very high level of accuracy because it gives them a sense of certainty that matches or exceeds that provided by the internal signal. But giving up the emotional reward of the internal signal is a high price to pay when the alternative is some sort of mechanical process that does not even claim high validity.</div>
<div class="s4P">This observation has an important implication for the improvement of judgment. Despite all the evidence in favor of mechanical and algorithmic prediction methods, and despite the rational calculus that clearly shows the value of incremental improvements in predictive accuracy, many decision makers will reject decision-making approaches that deprive them of the ability to exercise their intuition. As long as algorithms are not nearly perfect—and, in many domains, objective ignorance dictates that they will never be—human judgment will not be replaced. That is why it must be improved.</div>
<div class="s84">Speaking of Objective Ignorance</div>
<div class="s86">
<span class="class-2">“Wherever there is prediction, there is ignorance, and probably more of it than we think. Have we checked whether the experts we trust are more accurate than dart-throwing chimpanzees?”</span>
</div>
<div class="s88">
<span class="class-2">“When you trust your gut because of an internal signal, not because of anything you really know, you are in denial of your objective ignorance.”</span>
</div>
<div class="s88">
<span class="class-2">“Models do better than people, but not by much. Mostly, we find</span>
 <span id="page_139" class="class-2"></span>
<span class="class-2">mediocre human judgments and slightly better models. Still, better is good, and models are better.”</span>
</div>
<div class="s8A">
<span class="class-2">“We may never be comfortable using a model to make these decisions—we just need the internal signal to have enough confidence. So let’s make sure we have the best possible decision process.”</span>
</div>
</body>
</html>
