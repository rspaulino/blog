<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0036</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_290" class="s6B">
<a href="part0003.xhtml#a2X7">CHAPTER 25</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2X7">The Mediating Assessments Protocol</a>
</div>
<div class="s4M">
<span class="s2WY">S</span>
ome time ago, two of us (Kahneman and Sibony), together with our friend Dan Lovallo, described a method of decision making in organizations. We called the method, which was designed with noise mitigation as a primary objective, the <span class="s2WT-0">mediating assessments protocol.</span>
 It incorporates most of the decision hygiene strategies that we have introduced in the preceding chapters. The protocol can be applied broadly and whenever the evaluation of a plan or an option requires considering and weighting multiple dimensions. It can be used, and adapted in various ways, by organizations of all kinds, including diverse companies, hospitals, universities, and government agencies.</div>
<div class="s4P">We illustrate the protocol here with a stylized example that is a composite of several real cases: a fictitious corporation we’ll call Mapco. We will follow the steps Mapco takes as it studies the opportunity to make a major, transformative acquisition, and we will highlight how these differ from the usual steps a company takes in such a situation. As you will see, the differences are significant, but subtle—an inattentive observer might not even notice them.</div>
<div class="s7D">The First Meeting: Agreeing on the Approach</div>
<div class="s4M">The idea of acquiring Roadco, a competitor, had been percolating at Mapco, and had matured sufficiently so that the company’s leaders were contemplating a board meeting to discuss it. Joan Morrison, the CEO of Mapco, convened a meeting of the board’s strategy committee for a preliminary discussion of the possible acquisition <span id="page_291"></span>
and of what should be done to improve the board’s deliberations about it. Early in the meeting, Joan surprised the committee with a proposal:</div>
<div class="s4P">“I would like to propose that we try a new procedure for the board meeting where we will decide on the Roadco acquisition. The new procedure has an unappealing name, the mediating assessments protocol, but the idea is really quite simple. It is inspired by the similarity between the evaluation of a strategic option and the evaluation of a job candidate.</div>
<div class="s4P">“You are certainly familiar with the research that shows that structured interviews produce better results than unstructured ones, and more broadly with the idea that structuring a hiring decision improves it. You know that our HR department has adopted these principles for its hiring decisions. A vast amount of research shows that structure in interviews leads to much higher accuracy—unstructured interviews as we used to practice them don’t even come close.</div>
<div class="s4P">“I see a clear similarity between the evaluation of candidates and the evaluation of options in big decisions: <span class="s2WT-0">options are like candidates</span>
. And this similarity leads me to the idea that we should adapt the method that works for evaluating candidates to our task, which is to evaluate strategic options.”</div>
<div class="s4P">The committee members were initially puzzled by the analogy. The recruiting process, they argued, is a well-oiled machine that makes numerous, similar decisions and is not under severe time pressure. A strategic decision, on the other hand, requires a great deal of ad hoc work and must be made quickly. Some committee members made clear to Joan that they would be hostile to any proposal that delayed the decision. They were also worried about adding to the due-diligence requirements from Mapco’s research staff.</div>
<div class="s4P">Joan responded directly to these objections. She assured her colleagues that the structured process would not delay the decision. “This is all about setting the agenda for the board meeting in which we will discuss the deal,” she explained. “We should decide in advance on a list of assessments of different aspects of the deal, just as an interviewer starts with a job description that serves as a checklist of traits or attributes a candidate must possess. We will make sure the board discusses these assessments separately, one by <span id="page_292"></span>
one, just as interviewers in structured interviews evaluate the candidate on the separate dimensions in sequence. Then, and only then, will we turn to a discussion of whether to accept or reject the deal. This procedure will be a much more effective way to take advantage of the collective wisdom of the board.</div>
<div class="s4P">“If we agree on this approach, of course, it has implications for how the information should be presented and for how the deal team should work to prepare the meeting. That’s why I wanted to get your thoughts now.”</div>
<div class="s4P">One committee member, still skeptical, asked Joan what benefits the structure brought to the quality of decision making in hiring and why she believed these benefits would transfer to a strategic decision. Joan walked him through the logic. Using the mediating assessments protocol, she explained, maximizes the value of information by keeping the dimensions of the evaluation independent of each other. “The board discussions we usually have look a lot like unstructured interviews,” she observed. “We are constantly aware of the final goal of reaching a decision, and we process all the information in light of that goal. We start out looking for closure, and we achieve it as soon as we can. Just like a recruiter in an unstructured interview, we are at risk of using all the debate to confirm our first impressions.</div>
<div class="s4P">“Using a structured approach will force us to postpone the goal of reaching a decision until we have made all the assessments. We will take on the separate assessments as intermediate goals. This way, we will consider all the information available and make sure that our conclusion on one aspect of the deal does not change our reading on another, unrelated aspect.”</div>
<div class="s4P">The committee members agreed to try out the approach. But, they asked, what were the mediating assessments? Was there a predefined checklist that Joan had in mind? “No,” she replied. “That might be the case if we applied the protocol to a routine decision, but in this case, we need to define the mediating assessments ourselves. This is critically important: deciding on the major aspects of the acquisition that should be assessed is up to us.” The strategy committee agreed to meet again the next day to do that.</div>
<div class="s7D">The Second Meeting: Defining the Mediating Assessments</div>
<div id="page_293" class="s4M">“The first thing we are going to do,” Joan explained, “is draw up a comprehensive list of independent assessments about the deal. These will be assessed by Jeff Schneider’s research team. Our task today is to construct the list of assessments. It should be comprehensive in the sense that any relevant fact you can think of should find its place and should influence at least one of the assessments. And what I mean by ‘independent’ is that a relevant fact should preferably influence only one of the assessments, to minimize redundancy.”</div>
<div class="s4P">The group got to work and generated a long list of facts and data that seemed relevant. It then organized them into a list of assessments. The challenge, the participants soon discovered, was to make the list short, comprehensive, and composed of nonoverlapping assessments. But the task was manageable. Indeed, the group’s final list of seven assessments was superficially similar to the table of contents the board would expect in a regular report presenting an acquisition proposal. In addition to the expected financial modeling, the list included, for instance, an evaluation of the quality of the target’s management team and an assessment of the likelihood that the anticipated synergies would be captured.</div>
<div class="s4P">Some of the strategy committee members were disappointed that the meeting did not produce novel insights about Roadco. But, Joan explained, that was not the goal. The immediate objective was to brief the deal team in charge of studying the acquisition. Each assessment, she said, would be the subject of a different chapter in the deal team’s report and would be discussed separately by the board.</div>
<div class="s4P">The deal team’s mission, as Joan saw it, was not to tell the board what it thought of the deal as a whole—at least, not yet. It was to provide an objective, independent evaluation on each of the mediating assessments. Ultimately, Joan explained, each chapter in the deal team’s report should end with a rating that answers a simple question: “Leaving aside the weight we should give this topic in the final decision, how strongly does the evidence on this assessment argue for or against the deal?”</div>
<div class="s7D">The Deal Team</div>
<div id="page_294" class="s4M">The leader of the team in charge of evaluating the deal, Jeff Schneider, got his team together that afternoon to organize the work. The changes from the team’s usual way of working were not many, but he stressed their importance.</div>
<div class="s4P">First, he explained, the team’s analysts should try to make their analyses as objective as possible. The evaluations should be based on facts—nothing new about that—but they should also use an <span class="s2WT-0">outside view</span>
 whenever possible. Since the team members were unsure of what he meant by “outside view,” Jeff gave them two examples, using two of the mediating assessments Joan had identified. To evaluate the probability that the deal would receive regulatory approval, he said, they would need to start by finding out the <span class="s2WT-0">base rate,</span>
 the percentage of comparable transactions that are approved. This task would, in turn, require them to define a relevant <span class="s2WT-0">reference class,</span>
 a group of deals considered comparable enough.</div>
<div class="s4P">Jeff then explained how to evaluate the technological skills of the target’s product development department—another important assessment Joan had listed. “It is not enough to describe the company’s recent achievements in a fact-based way and to call them ‘good’ or ‘great.’ What I expect is something like, ‘This product development department is in the second quintile of its peer group, as measured by its recent track record of product launches.’ ” Overall, he explained, the goal was to make evaluations as comparative as possible, because relative judgments are better than absolute ones.</div>
<div class="s4P">Jeff had another request. In keeping with Joan’s instructions, he said, assessments should be as independent of one another as possible, to reduce the risk that one assessment would influence the others. Accordingly, he assigned different analysts to the different assessments, and he instructed them to work independently.</div>
<div class="s4P">Some of the analysts expressed surprise. “Isn’t teamwork better?” they asked him. “What’s the point of assembling a team if you don’t want us to communicate?”</div>
<div class="s4P">Jeff realized he needed to explain the need for independence. “You probably know about the halo effect in recruiting,” he said. “That is what happens when the general impression of a candidate influences your assessment of the candidate’s skills on a specific dimension. That’s what we are trying to avoid.” Since some of the analysts seemed to think that this effect was not a serious problem, <span id="page_295"></span>
Jeff used another analogy: “If you have four witnesses to a crime, would you let them talk to each other before testifying? Obviously not! You don’t want one witness to influence the others.” The analysts did not find the comparison particularly flattering, but it got the message across, Jeff thought.</div>
<div class="s4P">As it happened, Jeff did not have enough analysts to achieve the goal of perfectly independent assessments. Jane, an experienced member of the team, was charged with two assessments. Jeff chose the two to be as different from each other as possible, and he instructed Jane to complete the first assessment and prepare the report on it before turning to the other. Another concern was the evaluation of the quality of the management team; Jeff was worried that his analysts would struggle to dissociate their assessment of the team’s intrinsic quality from judgments about the company’s recent results (which the team would, of course, study in detail). To address this issue, Jeff asked an outside HR expert to weigh in on the quality of the management team. This way, he thought, he would obtain a more independent input.</div>
<div class="s4P">Jeff had another instruction that the team found somewhat unusual. Each chapter should focus on one assessment and, as requested by Joan, lead to a conclusion in the form of a rating. However, Jeff added, the analysts should include in each chapter all the relevant factual information about the assessment. “Don’t hide anything,” he instructed them. “The general tone of the chapter will be consistent with the proposed rating, of course, but if there is information that seems inconsistent or even contradictory with the main rating, don’t sweep anything under the rug. Your job is not to sell your recommendation. It is to represent the truth. If it is complicated, so be it—it often is.”</div>
<div class="s4P">In the same spirit, Jeff encouraged the analysts to be transparent about their level of confidence in each assessment. “The board knows that you do not have perfect information; it will help them if you tell them when you’re really in the dark. And if you run into something that really gives you pause—a potential deal breaker—you should, of course, report it immediately.”</div>
<div class="s4P">The deal team proceeded as instructed. Fortunately, it found no major deal breakers. It assembled a report for Joan and the board, covering all the assessments identified.</div>
<div id="page_296" class="s7D">The Decision Meeting</div>
<div class="s4M">As she read the team’s report to prepare for the decision meeting, Joan immediately noticed something important: while most of the assessments supported doing the deal, they did not paint a simple, rosy, all-systems-go picture. Some of the ratings were strong; others were not. These differences, she knew, were a predictable result of keeping the assessments independent of one another. When excessive coherence is kept in check, reality is not as coherent as most board presentations make it seem. “Good,” Joan thought. “These discrepancies between assessments will raise questions and trigger discussions. That’s just what we need to have a good debate in the board. The diverse results will not make the decision easier, for sure—but they will make it better.”</div>
<div class="s4P">Joan convened a meeting of the board to review the report and come to a decision. She explained the approach that the deal team followed, and she invited the board members to apply the same principle. “Jeff and his team have worked hard to keep the assessments independent of each other,” she said, “and our task now is to review them independently, too. This means we will consider each assessment separately, before we start discussing the final decision. We are going to treat each assessment as a distinct agenda item.”</div>
<div class="s4P">The board members knew that following this structured approach would be difficult. Joan was asking them not to form a holistic view of the deal before all assessments were discussed, but many of them were industry insiders. They <span class="s2WT-0">had</span>
 a view on Roadco. Not discussing it felt a bit artificial. Nevertheless, because they understood what Joan was trying to achieve, they agreed to play by her rules and refrain temporarily from discussing their overall views.</div>
<div class="s4P">To their surprise, the board members found that this practice was highly valuable. During the meeting, some of them even changed their mind about the deal (although no one would ever know, since they had kept their views to themselves). The way Joan ran the meeting played a large part: she used the <span class="s2WT-0">estimate-talk-estimate</span>
 method, which combines the advantages of deliberation and those of averaging independent opinions.</div>
<div class="s4P">Here is how she proceeded. On each assessment, Jeff, on <span id="page_297"></span>
behalf of the deal team, briefly summarized the key facts (which the board members had read in detail beforehand). Then Joan asked the board members to use a voting app on their phones to give their own rating on the assessment—either the same as the deal team’s proposed rating or a different one. The distribution of ratings was projected immediately on the screen, without identifying the raters. “This is not a vote,” Joan explained. “We are just taking the temperature of the room on each topic.” By getting an immediate read on each board member’s independent opinion before starting a discussion, Joan reduced the danger of social influence and information cascades.</div>
<div class="s4P">On some assessments, there was immediate consensus, but on others, the process revealed opposing views. Naturally, Joan managed the discussion to spend more time on the latter. She made sure that board members on each side of the divide spoke up, encouraging them to express their viewpoints with facts and arguments but also with nuance and humility. Once, when a board member who felt strongly about the deal got carried away, she reminded him that “we are all reasonable people and we disagree, so this must be a subject on which reasonable people can disagree.”</div>
<div class="s4P">When the discussion of an assessment drew to a close, Joan asked the board members to vote again on a rating. Most of the time, there was more convergence than in the initial round. The same sequence—a first estimate, a discussion, and a second estimate—was repeated for each assessment.</div>
<div class="s4P">Finally, it was time to reach a conclusion about the deal. To facilitate the discussion, Jeff showed the list of assessments on the whiteboard, with, for each assessment, the average of the ratings that the board had assigned to it. The board members were looking at the profile of the deal. How should they decide?</div>
<div class="s4P">One board member had a simple suggestion: use a straight average of the ratings. (Perhaps he knew about the superiority of mechanical aggregation over holistic, clinical judgment, as discussed in <a href="part0018.xhtml">chapter 9</a>
.) Another member, however, immediately objected that, in her view, some of the assessments should be given a much higher weight than others. A third person disagreed, suggesting a different hierarchy of the assessments.</div>
<div class="s4P">Joan interrupted the discussion. “This is not just about computing a simple combination of the assessment ratings,” she <span id="page_298"></span>
said. “We have delayed intuition, but now is the time to use it. What we need now is your judgment.”</div>
<div class="s4P">Joan did not explain her logic, but she had learned this lesson the hard way. She knew that, particularly with important decisions, people reject schemes that tie their hands and do not let them use their judgment. She had seen how decision makers game the system when they know that a formula will be used. They change the ratings to arrive at the desired conclusion—which defeats the purpose of the entire exercise. Furthermore, although this was not the case here, she remained alert to the possibility that decisive considerations could emerge that were not anticipated in the definition of assessments (the broken-leg factors discussed in <a href="part0019.xhtml">chapter 10</a>
). If such unanticipated deal breakers (or, conversely, deal clinchers) appeared, a purely mechanical decision process based on the average of the assessments might lead to a serious mistake.</div>
<div class="s4P">Joan also knew that letting the board members use their intuition at this stage was very different from having them use it earlier in the process. Now that the assessments were available and known to all, the final decision was safely anchored on these fact-based, thoroughly discussed ratings. A board member would need to come up with strong reasons to be against the deal while staring at a list of mediating assessments that mostly supported it. Following this logic, the board discussed the deal and voted on it, in much the same way all boards do.</div>
<div class="s7D">The Mediating Assessments Protocol in Recurring Decisions</div>
<div class="s4M">We have described the mediating assessments protocol in the context of a one-off, singular decision. But the procedure applies to recurring decisions, too. Imagine that Mapco is not making a single acquisition but is a venture capital fund that makes repeated investments in start-ups. The protocol would be just as applicable and the story would be much the same, with just two twists that, if anything, make it simpler.</div>
<div class="s4P">First, the initial step—defining the list of mediating assessments—needs to be done only once. The fund has investment <span id="page_299"></span>
criteria, which it applies to all its prospective investments: these are the assessments. There is no need to reinvent them each time.</div>
<div class="s4P">Second, if the fund makes many decisions of the same type, it can use its experience to calibrate its judgments. Consider, for instance, an assessment that every fund will want to make: evaluating the quality of the management team. We suggested that such evaluations should be made relative to a reference class. Perhaps you sympathized with the analysts of Mapco: gathering data about comparable companies, in addition to evaluating a specific target, is challenging.</div>
<div class="s4P">Comparative judgments become much easier in the context of a recurring decision. If you have evaluated the management teams of dozens, even hundreds of companies, you can use this shared experience as a reference class. A practical way to do this is to create a case scale defined by anchor cases. You might say, for instance, that the target management team is “as good as the management team of ABC Company when we acquired it” but not quite “as good as the management team of DEF Company.” The anchor cases must, of course, be known to all the participants (and periodically updated). Defining them requires an up-front investment of time. But the value of this approach is that relative judgments (comparing this team to the ones at ABC and DEF) are much more reliable than are absolute ratings on a scale defined by numbers or adjectives.</div>
<div class="s7D">What the Protocol Changes</div>
<div class="s4M">For ease of reference, we summarize the main changes that the mediating assessments protocol entails in <a href="part0036.xhtml#a2YX">table 4</a>
.</div>
<div class="sRD">
<div id="a2YX">
<span class="s2WV-0">Table 4:</span>
 <span class="s2WT-0">Main steps of the mediating assessments protocol</span>
</div>
</div>
<div class="s1PC">
<span class="s2YA">1.</span>
 <span class="class-1">At the beginning of the process, structure the decision into mediating assessments.</span>
 <span class="s2WT-2">(For recurring judgments, this is done only once.)</span>
</div>
<div class="s1PE">
<span class="s2YA">2.</span>
 <span class="class-1">Ensure that whenever possible, mediating assessments use an outside view</span>
<span class="s2WT-2">. (For recurring judgments: use relative judgments, with a case scale if possible.)</span>
</div>
<div class="s1PE">
<span class="s2YA">3.</span>
<span id="page_300" class="class-1"></span>
 <span class="class-1">In the analytical phase, keep the assessments as independent of one another as possible.</span>
</div>
<div class="s1PE">
<span class="s2YA">4.</span>
 <span class="class-1">In the decision meeting, review each assessment separately.</span>
</div>
<div class="s1PE">
<span class="s2YA">5.</span>
 <span class="class-1">On each assessment, ensure that participants make their judgments individually; then use the estimate-talk-estimate method.</span>
</div>
<div class="s1PE">
<span class="s2YA">6.</span>
 <span class="class-1">To make the final decision, delay intuition, but don’t ban it.</span>
</div>
<div class="s1XR"></div>
<div class="s4P">You may have recognized here an implementation of several of the decision hygiene techniques we presented in the preceding chapters: sequencing information, structuring the decision into independent assessments, using a common frame of reference grounded in the outside view, and aggregating the independent judgments of multiple individuals. By implementing these techniques, the mediating assessments protocol aims to change the decision <span class="s2WT-0">process</span>
 to introduce as much decision hygiene as possible.</div>
<div class="s4P">No doubt this emphasis on process, as opposed to the content of decisions, may raise some eyebrows. The reactions of the research team members and the board members, as we have described them, are not unusual. Content is specific; process is generic. Using intuition and judgment is fun; following process is not. Conventional wisdom holds that good decisions—especially the very best ones—emerge from the insight and creativity of great leaders. (We especially like to believe this when we are the leader in question.) And to many, the word <span class="s2WT-0">process</span>
 evokes bureaucracy, red tape, and delays.</div>
<div class="s4P">Our experience with companies and government agencies that have implemented all or some of the components of the protocol suggests that these concerns are misguided. To be sure, adding complexity to the decision-making processes of an organization that is already bureaucratic will not make things better. But decision hygiene need not be slow and certainly doesn’t need to be bureaucratic. On the contrary, it promotes challenge and debate, not the stifling consensus that characterizes bureaucracies.</div>
<div class="s4P">The case for decision hygiene is clear. Leaders in business and in the public sector are usually entirely unaware of noise in their largest and most important decisions. As a result, they take no <span id="page_301"></span>
specific measures to reduce it. In that respect, they are just like the recruiters who continue to rely on unstructured interviews as their sole personnel selection tool: oblivious to the noise in their own judgment, more confident in its validity than they should be, and unaware of procedures that could improve it.</div>
<div class="s4P">Handwashing does not prevent all diseases. Likewise, decision hygiene will not prevent all mistakes. It will not make every decision brilliant. But like handwashing, it addresses an invisible yet pervasive and damaging problem. Wherever there is judgment, there is noise, and we propose decision hygiene as a tool to reduce it.</div>
<div class="s84">Speaking of the Mediating Assessments Protocol</div>
<div class="s86">
<span class="class-2">“We have a structured process to make hiring decisions. Why don’t we have one for strategic decisions? After all, options are like candidates.”</span>
</div>
<div class="s88">
<span class="class-2">“This is a difficult decision. What are the mediating assessments it should be based on?”</span>
</div>
<div class="s8A">
<span class="class-2">“Our intuitive, holistic judgment about this plan is very important—but let’s not discuss it yet. Our intuition will serve us much better once it is informed by the separate assessments we have asked for.”</span>
</div>
</body>
</html>
