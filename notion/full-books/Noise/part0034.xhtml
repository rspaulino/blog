<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0034</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_267" class="s6B">
<a href="part0003.xhtml#a2XY">CHAPTER 23</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2XY">Defining the Scale in Performance Ratings</a>
</div>
<div class="s4M">
<span class="s2WY">L</span>
et’s start with an exercise. Take three people you know; they might be friends or colleagues. Rate them on a scale of 1 to 5, where 1 is the lowest and 5 is the highest, in terms of three characteristics: kindness, intelligence, and diligence. Now ask someone who knows them well—your spouse, best friend, or closest colleague—to do the same thing with respect to the same three people.</div>
<div class="s4P">There is a good chance that on some of the ratings, you and the other rater came up with different numbers. If you (and your counterpart) are willing, please discuss the reasons for the differences. You might find that the answer lies in how you used the scale—what we have called level noise. Perhaps you thought a 5 requires something truly extraordinary, whereas the other rater thought that it merely requires something unusually good. Or perhaps you differed because of your differing views of the people being rated: your understanding of whether they are kind, and how exactly to define that virtue, might be different from that of the other rater.</div>
<div class="s4P">Now imagine that for the three people you rated, a promotion or bonus is at stake. Suppose that you and the other rater are engaged in performance ratings at a company that values kindness (or collegiality), intelligence, and diligence. Would there be a difference between your ratings? Would it be as large as in the earlier exercise? Even larger? However those questions are answered, differences in policies and scaling are likely to produce noise. And in fact, that is what is pervasively observed in performance ratings across organizational settings.</div>
<div id="page_268" class="s7D">A Judgment Task</div>
<div class="s4M">In almost all large organizations, performance is formally evaluated on a regular basis. Those who are rated do not enjoy the experience. As one newspaper headline put it, “Study Finds That Basically Every Single Person Hates Performance Reviews.” Every single person also knows (we think) that performance reviews are subject to both bias and noise. But most people do not know just how noisy they are.</div>
<div class="s4P">In an ideal world, evaluating people’s performance would not be a judgment task; objective facts would be sufficient to determine how well people are doing. But most modern organizations have little in common with Adam Smith’s pin factory, in which every worker had a measurable output. What would that output be for a chief financial officer or for a head of research? Today’s knowledge workers balance multiple, sometimes contradictory objectives. Focusing on only one of them might produce erroneous evaluations and have harmful incentive effects. The number of patients a doctor sees every day is an important driver of hospital productivity, for example, but you would not want physicians to focus single-mindedly on that indicator, much less to be evaluated and rewarded only on that basis. Even quantifiable performance metrics—say, sales for a salesperson or number of lines of code written for a programmer—must be evaluated in context: not all customers are equally difficult to serve, and not all software development projects are identical. In light of these challenges, many people cannot be evaluated entirely on the basis of objective performance metrics. Hence the ubiquity of judgment-based performance reviews.</div>
<div class="s7D">One-Quarter Signal, Three-Quarters Noise</div>
<div class="s4M">Thousands of research articles have been published on the practice of performance appraisals. Most researchers find that such appraisals are exceedingly noisy. This sobering conclusion comes mostly from studies based on 360-degree performance reviews, in which multiple raters provide input on the same person being rated, usually on multiple dimensions of performance. When this analysis is conducted, the result is not pretty. Studies often find that true variance, that is, variance attributable to the person’s performance, accounts for no more than 20 to 30% of the total variance. The rest, <span id="page_269"></span>
70 to 80% of the variance in the ratings, is system noise.</div>
<div class="s4P">Where does this noise come from? Thanks to multiple studies of variance in job performance ratings, we know that all the components of system noise are present.</div>
<div class="s4P">These components are quite easy to picture in the context of a performance rating. Consider two raters, Lynn and Mary. If Lynn is lenient and Mary tough, in the sense that Lynn gives higher ratings than Mary does, on average, to all people being evaluated, then we have level noise. As noted in our discussion of judges, this noise may mean either that Lynn and Mary form truly different impressions or that the two raters merely use the rating scale differently to express the same impression.</div>
<div class="s4P">Now, if Lynn is evaluating you and happens to have a distinctly poor opinion of you and your contributions, her general leniency may be offset by her idiosyncratic (and negative) reaction to you. This is what we have called a stable pattern: a specific rater’s reaction to a specific person being rated. Because the pattern is unique to Lynn (and to her judgment of you), it is a source of pattern noise.</div>
<div class="s4P">Finally, Mary may have discovered that someone dented her car in the company parking lot just before she filled in a rating form, or Lynn may just have received her own, surprisingly generous, bonus, which put her in an unusually good mood as she evaluated your performance. Such events may, of course, produce occasion noise.</div>
<div class="s4P">Different studies come to different conclusions on the breakdown of system noise into these three components (level, pattern, and occasion), and we can certainly imagine reasons why it should vary from one organization to the next. But all forms of noise are undesirable. The basic message that emerges from this research is a simple one: most ratings of performance have much less to do with the performance of the person being rated than we would wish. As one review summarizes it, “the relationship between job performance and ratings of job performance is likely to be weak or at best uncertain.”</div>
<div class="s4P">In addition, there are many reasons why ratings in organizations might not reflect the rater’s perception of an employee’s true performance. For example, raters might not in fact attempt to evaluate performance accurately but might rate people <span id="page_270"></span>
“strategically.” Among other motives, the evaluators might intentionally inflate a rating to avoid a difficult feedback conversation, to favor a person who is seeking a long-awaited promotion, or even, paradoxically, to get rid of an underperforming team member who needs a good evaluation to be allowed to transfer to another division.</div>
<div class="s4P">These strategic calculations certainly affect ratings, but they are not the only source of noise. We know this thanks to a sort of natural experiment: some 360-degree feedback systems are used solely for developmental purposes. With these systems, the respondents are told that the feedback will not be used for evaluation purposes. To the extent that the raters actually believe what they are told, this approach discourages them from inflating—or deflating—ratings. As it turns out, the developmental review does make a difference in the quality of the feedback, but system noise remains high and still accounts for much more variance than does the performance of the person being rated. Even when the feedback is purely developmental, ratings remain noisy.</div>
<div class="s7D">A Problem Long Recognized but Not Solved</div>
<div class="s4M">If performance rating systems are so badly broken, the people who measure performance should take notice and improve them. Indeed, over the past several decades, organizations have experimented with countless reforms to those systems. The reforms have employed some of the noise-reduction strategies we have outlined. In our view, much more could be done.</div>
<div class="s4P">Almost all organizations use the noise-reduction strategy of <span class="s2WT-0">aggregation.</span>
 Aggregate ratings are often associated with 360-degree rating systems, which became the standard in large corporations in the 1990s. (The journal <span class="s2WT-0">Human Resources Management</span>
 had a special issue on 360-degree feedback in 1993.)</div>
<div class="s4P">While averaging ratings from several raters should help to reduce system noise, it is worth noting that 360-degree feedback systems were not invented as a remedy for that problem. Their primary purpose is to measure much more than what a boss sees. When your peers and subordinates, and not just your boss, are asked to contribute to your performance evaluation, the nature of what is <span id="page_271"></span>
valued is changed. The theory is that this shift is for the better, because today’s jobs entail more than pleasing your boss. The rise in popularity of 360-degree feedback coincided with the generalization of fluid, project-based organizations.</div>
<div class="s4P">Some evidence suggests that 360-degree feedback is a useful tool in that it predicts objectively measurable performance. Unfortunately, the use of this feedback system has created its own problems. As computerization made it effortless to add more questions to feedback systems, and as the proliferation of multiple corporate objectives and constraints added dimensions to job descriptions, many feedback questionnaires became absurdly complex. Overengineered questionnaires abound (one example involves forty-six ratings on eleven dimensions for each rater and person being rated). It would take a superhuman rater to recall and process accurate, relevant facts about numerous people being evaluated on so many dimensions. In some ways, this overly complicated approach is not only useless but also pernicious. As we have seen, the halo effect implies that supposedly separate dimensions will in fact not be treated separately. A strong positive or negative rating on one of the first questions will tend to pull answers to subsequent questions in the same direction.</div>
<div class="s4P">Even more importantly, the development of 360-degree systems has exponentially increased the amount of time devoted to providing feedback. It is not uncommon for middle managers to be asked to complete dozens of questionnaires on their colleagues at all levels—and sometimes on their counterparts in other organizations, because many companies now request feedback from customers, vendors, and other business partners. However well intentioned, this explosion in the demands placed on time-constrained raters cannot be expected to improve the quality of the information they supply. In this case, the reduction of noise may not be worth the cost—a problem that we will discuss in <a href="part0037.xhtml">part 6</a>
.</div>
<div class="s4P">Finally, 360-degree systems are not immune to a near-universal disease of all performance measurement systems: creeping ratings inflation. One large industrial company once observed that 98% of its managers had been rated as “fully meeting expectations.” When almost everyone receives the highest possible rating, it is fair to question the value of these ratings.</div>
<div id="page_272" class="s7D">In Praise of Relative Judgments</div>
<div class="s4M">A theoretically effective solution to the problem of ratings inflation is to introduce some standardization in ratings. One popular practice that aims to do this is <span class="s2WT-0">forced ranking.</span>
 In a forced ranking system, raters are not only prevented from giving everyone the highest possible rating but also forced to abide by a predetermined distribution. Forced ranking was advocated by Jack Welch when he was CEO of General Electric, as a way to stop inflation in ratings and to ensure “candor” in performance reviews. Many companies adopted it, only to abandon it later, citing undesirable side effects on morale and teamwork.</div>
<div class="s4P">Whatever their flaws, rankings are less noisy than ratings. We saw in the example of punitive damages that there is much less noise in relative judgments than in absolute ones, and this relationship has been shown to apply in performance ratings, too.</div>
<div class="sE6">
<div class="sK">
<div class="sH-0"><img src="rsrc31V.jpg" alt="" id="a2YR" class="sH-1" />
</div>
</div>
</div>
<div class="sE6">
<div class="sK">
<div class="sH-0"><img src="rsrc31W.jpg" alt="" class="sH-1" />
</div>
<div class="sE3">F<span class="s2WW">IGURE 17:</span>
 <span class="s2WT-0">Examples of absolute and relative rating scales</span>
</div>
</div>
</div>
<div class="s4P">To appreciate why, consider <a href="part0034.xhtml#a2YR">figure 17</a>
, which shows two examples of scales for evaluating employees. Panel A, in which an employee is rated on an absolute scale, requires what we have called a matching operation: finding the score that most closely matches your impression of the employee’s “work quality.” Panel B, by contrast, requires each individual to be compared with a group of others on a specific dimension—safety. The supervisor is asked to <span id="page_273"></span>
state the rank (or percentile) of an employee in a specified population, using a percentile scale. We can see that a supervisor has placed three employees on this common scale.</div>
<div class="s4P">The approach in panel B has two advantages. First, rating all employees on one dimension at a time (in this example, safety) exemplifies a noise-reduction strategy we will discuss in more detail in the next chapter: <span class="s2WT-0">structuring</span>
 a complex judgment into several dimensions. Structuring is an attempt to limit the halo effect, which usually keeps the ratings of one individual on different dimensions within a small range. (Structuring, of course, works only if the ranking is done on each dimension separately, as in this example: ranking employees on an ill-defined, aggregate judgment of “work quality” would not reduce the halo effect.)</div>
<div class="s4P">Second, as we discussed in <a href="part0025.xhtml">chapter 15</a>
, a ranking reduces both pattern noise and level noise. You are less likely to be inconsistent (and to create pattern noise) when you compare the performance of two members of your team than when you separately give each one a grade. More importantly, rankings mechanically eliminate level noise. If Lynn and Mary are evaluating the same group of twenty employees, and Lynn is more lenient than Mary, their average ratings will be different, but their average rankings will not. A lenient ranker and a tough ranker use the same ranks.</div>
<div class="s4P">Indeed, noise reduction is the main stated objective of forced ranking, which ensures that all raters have the same mean and the same distribution of evaluations. Rankings are “forced” when a distribution of ratings is mandated. For instance, a rule might state that no more than 20% of the people being rated can be put in the top category and that no less than 15% can be put in the bottom one.</div>
<div class="s7D">Rank but Do Not Force</div>
<div class="s4M">In principle, therefore, forced ranking should bring about much-needed improvements. Yet it often backfires. We do not intend here to review all its possible unwanted effects (which are often related to poor implementation rather than principle). But two issues with forced ranking systems offer some general lessons.</div>
<div class="s4P">The first is the confusion between absolute and relative <span id="page_274"></span>
performance. It is certainly impossible for 98% of the managers of any company to be in the top 20%, 50%, or even 80% of their peer group. But it is not impossible that they all “meet expectations,” if these expectations have been defined ex ante <span class="s2WT-0">and in absolute terms</span>
.</div>
<div class="s4P">Many executives object to the notion that nearly all employees can meet expectations. If so, they argue, the expectations must be too low, perhaps because of a culture of complacency. Admittedly this interpretation may be valid, but it is also possible that most employees really do meet <span class="s2WT-0">high</span>
 expectations. Indeed, this is exactly what we would expect to find in a high-performance organization. You would not sneer at the leniency of the National Aeronautics and Space Administration’s performance management procedures if you heard that all the astronauts on a successful space mission have fully met expectations.</div>
<div class="s4P">The upshot is that a system that depends on relative evaluations is appropriate only if an organization cares about relative performance. For example, relative ratings might make sense when, regardless of people’s absolute performance, only a fixed percentage of them can be promoted—think of colonels being evaluated for promotion to general. But forcing a relative ranking on what purports to measure an <span class="s2WT-0">absolute</span>
 level of performance, as many companies do, is illogical. And mandating that a set percentage of employees be rated as failing to meet (absolute) expectations is not just cruel; it is absurd. It would be foolish to say that 10% of an elite unit of the army must be graded “unsatisfactory.”</div>
<div class="s4P">The second problem is that the forced distribution of the ratings is assumed to reflect the distribution of the underlying true performances—typically, something close to a normal distribution. Yet even if the distribution of performances in the population being rated is known, the same distribution may not be reproduced in a smaller group, such as those assessed by a single evaluator. If you randomly pick ten people from a population of several thousand, there is no guarantee that exactly two of them will belong to the top 20% of the general population. (“No guarantee” is an understatement: the probability that this will be the case is just 30%.) In practice, the problem is even worse, because the composition of teams is not random. Some units may be staffed almost entirely with high performers, and others with subpar employees.</div>
<div id="page_275" class="s4P">Inevitably, forced ranking in such a setting is a source of error and unfairness. Suppose that one rater’s team is composed of five people whose performances are indistinguishable. Forcing a differentiated distribution of ratings on this undifferentiated reality does not reduce error. It increases it.</div>
<div class="s4P">Critics of forced ranking have often focused their attacks on the principle of ranking, which they decry as brutal, inhumane, and ultimately counterproductive. Whether or not you accept these arguments, the fatal flaw of forced ranking is not the “ranking,” but the “forced.” Whenever judgments are forced onto an inappropriate scale, either because a relative scale is used to measure an absolute performance or because judges are forced to distinguish the indistinguishable, the choice of the scale mechanically adds noise.</div>
<div class="s7D">What’s Next?</div>
<div class="s4M">In light of all the efforts that organizations have made to improve performance measurement, it is an understatement to say that the results have been disappointing. As a result of those efforts, the cost of performance evaluations skyrocketed. In 2015, Deloitte calculated that it was spending 2 million hours each year evaluating its sixty-five thousand people. Performance reviews continue to be one of the most dreaded rituals of organizations, hated almost as much by those who have to perform them as by those who receive them. One study found that a staggering 90% of managers, employees, and HR heads believe that their performance management processes fail to deliver the results they expected. Research has confirmed what most managers have experienced. Although performance feedback, when associated with a development plan for the employee, can bring about improvements, performance ratings as they are most often practiced demotivate as often as they motivate. As one review article summarized, “No matter what has been tried over decades to improve [performance management] processes, they continue to generate inaccurate information and do virtually nothing to drive performance.”</div>
<div class="s4P">In despair, a small but growing number of companies are now considering the radical option of eliminating evaluation systems altogether. Proponents of this “performance management <span id="page_276"></span>
revolution,” including many technology companies, some professional services organizations, and a handful of companies in traditional sectors, aim to focus on developmental, future-oriented feedback rather than on evaluative, backward-looking assessment. A few have even made their evaluations numberless, which means that they abandon traditional performance ratings.</div>
<div class="s4P">For companies that are not giving up on performance ratings (and they are the overwhelming majority), what can be done to improve them? One noise-reduction strategy has to do, again, with picking the right scale. The aim is to ensure a <span class="s2WT-0">common frame of reference.</span>
 Research suggests that a combination of improved rating formats and training of the raters can help achieve more consistency between raters in their use of the scale.</div>
<div class="s4P">At a minimum, performance rating scales must be anchored on descriptors that are sufficiently specific to be interpreted consistently. Many organizations use <span class="s2WT-0">behaviorally anchored rating scales</span>
 in which each degree on the scale corresponds to a description of specific behaviors. The left panel of <a href="part0034.xhtml#page_277">figure 18</a>
 provides an example.</div>
<div class="s4P">Evidence suggests, however, that behaviorally anchored rating scales are not sufficient to eliminate noise. A further step, <span class="s2WT-0">frame-of-reference training,</span>
 has been shown to help ensure consistency between raters. In this step, raters are trained to recognize different dimensions of performance. They practice rating performance using videotaped vignettes and then learn how their ratings compare with “true” ratings provided by experts. The performance vignettes act as reference cases; each vignette defines an anchor point on the performance scale, which becomes a <span class="s2WT-0">case scale,</span>
 such as the one shown on the right panel of <a href="part0034.xhtml#page_277">figure 18</a>
.</div>
<div class="s4P">With a case scale, each rating of a new individual is a comparison with the anchor cases. It becomes a relative judgment. Because comparative judgments are less susceptible to noise than ratings are, case scales are more reliable than scales that use numbers, adjectives, or behavioral descriptions.</div>
<div class="sG5">
<div class="sK">
<div class="sH-0"><img src="rsrc31X.jpg" alt="" id="page_277" class="sH-1" />
</div>
<div class="sG2">F<span class="s2WW">IGURE 18:</span>
 <span class="s2X5">Example of a behaviorally anchored rating scale (left) and case scale (right)</span>
</div>
</div>
</div>
<div class="s4P">Frame-of-reference training has been known for decades and provides demonstrably less noisy and more accurate ratings. Yet it has gained little ground. It is easy to guess why. Frame-of-reference training, case scales, and other tools that pursue the same goals are complex and time-consuming. To be valuable, they usually need to be customized for the company and even for the unit conducting the evaluations, and they must be frequently updated as job requirements evolve. These tools require a company to add to its already-large investment in its performance management systems. Current fashion goes in the opposite direction. (In <a href="part0037.xhtml">part 6</a>
, we shall have more to say about the costs of reducing noise.)</div>
<div class="s4P">In addition, any organization that tames the noise attributable to raters also reduces their ability to influence ratings in pursuit of their own goals. Requiring managers to undergo additional rater training, to invest more effort in the rating process, and to give up some of the control they have over outcomes is certain to generate considerable resistance. Tellingly, the majority of studies of frame-of-reference rater training have so far been conducted on students, not on actual managers.</div>
<div class="s4P">The large subject of performance evaluation raises many questions, both practical and philosophical. Some people ask, for instance, to what extent the notion of individual performance is meaningful in today’s organizations, where outcomes often depend on how people interact with one another. If we believe the notion is indeed meaningful, we must wonder how levels of individual performance are distributed among people in a given organization—for instance, whether performance follows a normal distribution or whether there exists “star talent” making a hugely disproportionate contribution. And if your goal is to bring out the best in people, you <span id="page_278"></span>
can reasonably ask whether measuring individual performance and using that measurement to motivate people through fear and greed is the best approach (or even an effective one).</div>
<div class="s4P">If you are designing or revising a performance management system, you will need to answer these questions and many more. Our aspiration here is not to examine these questions but to make a more modest suggestion: if you do measure performance, your performance ratings have probably been pervaded by system noise and, for that reason, they might be essentially useless and quite possibly counterproductive. Reducing this noise is a challenge that cannot be solved by simple technological fixes. It requires clear thinking about the judgments that raters are expected to make. Most likely, you will find that you can improve judgments by clarifying the rating scale and training people to use it consistently. This noise-reduction strategy is applicable in many other fields.</div>
<div class="s84">Speaking of Defining the Scale</div>
<div class="s86">
<span class="class-2">“We spend a lot of time on our performance ratings, and yet the results are one-quarter performance and three-quarters system noise.”</span>
</div>
<div class="s88">
<span class="class-2">“We tried 360-degree feedback and forced ranking to address this problem, but we may have made things worse.”</span>
</div>
<div class="s8A">
<span class="class-2">“If there is so much level noise, it is because different raters have completely different ideas of what ‘good’ or ‘great’ means. They will only agree if we give them concrete cases as anchors on the rating scale.”</span>
</div>
</body>
</html>
