<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0012</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_44" class="s6B">
<a href="part0003.xhtml#a2XF">CHAPTER 4</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2XF">Matters of Judgment</a>
</div>
<div class="s4M">
<span class="s2WY">T</span>
his book is about professional judgments, broadly understood, and it assumes that whoever makes such a judgment is competent and aiming to get it right. However, the very concept of judgment involves a reluctant acknowledgment that you can never be certain that a judgment is right.</div>
<div class="s4P">Consider the phrases “matter of judgment” or “it’s a judgment call.” We do not consider the proposition that the sun will rise tomorrow or that the formula of sodium chloride is NaCl to be matters of judgment, because reasonable people are expected to agree perfectly on them. A matter of judgment is one with some uncertainty about the answer and where we allow for the possibility that reasonable and competent people might disagree.</div>
<div class="s4P">But there is a limit to how much disagreement is admissible. Indeed, the word <span class="s2WT-0">judgment</span>
 is used mainly where people believe they should agree. Matters of judgment differ from matters of opinion or taste, in which unresolved differences are entirely acceptable. The insurance executives who were shocked by the result of the noise audit would have no problem if claims adjusters were sharply divided over the relative merits of the Beatles and the Rolling Stones, or of salmon and tuna.</div>
<div class="s4P">Matters of judgment, including professional judgments, occupy a space between questions of fact or computation on the one hand and matters of taste or opinion on the other. They are defined by the <span class="s2WT-0">expectation of bounded disagreement</span>
.</div>
<div class="s4P">Exactly how much disagreement is acceptable in a judgment is itself a judgment call and depends on the difficulty of the problem. Agreement is especially easy when a judgment is absurd. Judges who differ widely in the sentences they set in a run-of-the-mill fraud case <span id="page_45"></span>
will concur that a fine of one dollar and a life sentence are both unreasonable. Judges at wine competitions differ greatly on which wines should get medals, but are often unanimous in their contempt for the rejects.</div>
<div class="s7D">The Experience of Judgment: An Example</div>
<div class="s4M">Before we further discuss the experience of judgment, we now ask you to make one yourself. You will absorb more from the rest of this chapter if you do this exercise and carry it out to completion.</div>
<div class="sC1">
<span class="s2WT-0">Imagine that you are a member of a team charged with evaluating candidates for the position of chief executive in a moderately successful regional financial firm that faces increasing competition. You are asked to assess the probability that the following candidate will be successful after two years on the job.</span>
 Successful <span class="s2WT-0">is defined simply as the candidate’s having kept the CEO job at the end of the two years. Express the probability on a scale from 0 (impossible) to 100 (certain).</span>
</div>
<div class="sC3">
<span class="class-6">Michael Gambardi is thirty-seven years old. He has held several positions since he graduated from Harvard Business School twelve years ago. Early on, he was a founder and an investor in two start-ups that failed without attracting much financial support. He then joined a large insurance company and quickly rose to the position of regional chief operating officer for Europe. In that post, he initiated and managed an important improvement in the timely resolution of claims. He was described by colleagues and subordinates as effective but also as domineering and abrasive, and there was significant turnover of executives during his tenure. Colleagues and subordinates also attest to his integrity and willingness to take responsibility for failures. For the last two years, he has served as CEO of a medium-sized financial company that was initially at risk of failing. He stabilized the company, where he is</span>
 <span id="page_46" class="class-6"></span>
<span class="class-6">considered successful though difficult to work with. He has indicated an interest in moving on. Human resources specialists who interviewed him a few years ago gave him superior grades for creativity and energy but also described him as arrogant and sometimes tyrannical.</span>
</div>
<div class="sC5">
<span class="class-4">Recall that Michael is a candidate for a CEO position in a regional financial firm that is moderately successful and that faces increasing competition. What is the probability that Michael, if hired, will still be in his job after two years? Please decide on a specific number in the range of 0 to 100 before reading on. Read the description again if you need to.</span>
</div>
<div class="s4P">If you engaged in the task seriously, you probably found it difficult. There is a mass of information, much of it seemingly inconsistent. You had to struggle to form the coherent impression that you needed to produce a judgment. In constructing that impression, you focused on some details that appeared important and you very likely ignored others. If asked to explain your choice of a number, you would mention a few salient facts but not enough of them for a full accounting of your judgment.</div>
<div class="s4P">The thought process you went through illustrates several features of the mental operation we call judgment:</div>
<div class="s5K">
<span class="class-3">• Of all the cues provided by the description (which are only a subset of what you might need to know), you attended to some more than others without being fully aware of the choices you made. Did you notice that Gambardi is an Italian name? Do you remember the school he attended? This exercise was designed to overload you so that you could not easily recover all the details of the case. Most likely, your recollection of what we presented would be different from that of other readers. Selective attention and selective recall are a source of variability across people.</span>
</div>
<div class="s5N">
<span class="class-3">• Then, you informally integrated these cues into an overall impression of Gambardi’s prospects. The key word here is</span>
 <span class="s2WT-4">informally</span>
<span class="class-3">. You did not construct a plan for answering the question. Without being fully aware of what you were doing,</span>
 <span id="page_47" class="class-3"></span>
<span class="class-3">your mind worked to construct a coherent impression of Michael’s strengths and weaknesses and of the challenges he faces. The informality allowed you to work quickly. It also produces variability: a formal process such as adding a column of numbers guarantees identical results, but some noise is inevitable in an informal operation.</span>
</div>
<div class="s5W">
<span class="class-3">• Finally, you converted this overall impression into a number on a probability scale of success. Matching a number between 0 and 100 to an impression is a remarkable process, to which we will return in</span>
 <a href="part0024.xhtml" class="class-3">
<span class="class-3">chapter 14</span>
</a>
<span class="class-3">. Again, you do not know exactly why you responded as you did. Why did you choose, say, 65 rather than 61 or 69? Most likely, at some point, a number came to your mind. You checked whether that number felt right, and if it did not, another number came to mind. This part of the process is also a source of variability across people.</span>
</div>
<div class="s4P">Since each of these three steps in a complex judgment process entails some variability, we should not be surprised to find a lot of noise in answers about Michael Gambardi. If you ask a few friends to read the case, you will probably find that your estimates of his probability of success are scattered widely. When we showed the case to 115 MBA students, their estimates of Gambardi’s probability of success ranged from 10 to 95. That is a great deal of noise.</div>
<div class="s4P">Incidentally, you may have noticed that the stopwatch exercise and the Gambardi problem illustrate two types of noise. The variability of judgments over successive trials with the stopwatch is noise within a single judge (yourself), whereas the variability of judgments of the Gambardi case is noise between different judges. In measurement terms, the first problem illustrates <span class="s2WT-0">within-person</span>
 reliability, and the second illustrates <span class="s2WT-0">between-person</span>
 reliability.</div>
<div class="s7D">What Judgment Aims to Achieve: The Internal Signal</div>
<div class="s4M">Your answer to the Gambardi question is a predictive judgment, as we have defined the term. However, it differs in important ways from other judgments that we call predictive, including tomorrow’s peak temperature in Bangkok, the result of tonight’s football game, or the <span id="page_48"></span>
outcome of the next presidential election. If you disagree with a friend about these problems, you will, at some point, find out who is right. But if you disagree about Gambardi, time will <span class="s2WT-0">not</span>
 tell who was right, for a simple reason: Gambardi does not exist.</div>
<div class="s4P">Even if the question referred to a real person and we knew the outcome, a single probability judgment (other than 0 or 100%) cannot be confirmed or disconfirmed. The outcome does not reveal what the ex ante probability was. If an event that was assigned a probability of 90% fails to happen, the judgment of probability was not necessarily a bad one. After all, outcomes that are just 10% likely to happen end up happening 10% of the time. The Gambardi exercise is an example of a <span class="s2WT-0">nonverifiable</span>
 predictive judgment, for two separate reasons: Gambardi is fictitious and the answer is probabilistic.</div>
<div class="s4P">Many professional judgments are nonverifiable. Barring egregious errors, underwriters will never know, for instance, whether a particular policy was overpriced or underpriced. Other forecasts may be nonverifiable because they are conditional. “If we go to war, we will be crushed” is an important prediction, but it is likely to remain untested (we hope). Or forecasts may be too long term for the professionals who make them to be brought to account—like, for instance, an estimate of mean temperatures by the end of the twenty-first century.</div>
<div class="s4P">Did the nonverifiable nature of the Gambardi task change how you approached it? Did you, for instance, ask yourself whether Gambardi was real or fictitious? Did you wonder whether the outcome would be revealed later in the text? Did you reflect on the fact that, even if that were the case, the revelation would not give you the answer to the question you were facing? Probably not, because these considerations did not seem relevant when you answered the question.</div>
<div class="s4P">Verifiability does not change the experience of judgment. To some degree, you might perhaps think harder about a problem whose answer will be revealed soon, because the fear of being exposed concentrates the mind. Conversely, you might refuse to give much thought to a problem so hypothetical as to be absurd (“If Gambardi had three legs and could fly, would he be a better CEO?”). But, by and large, you address a plausible, hypothetical problem in much the same way that you tackle a real one. This similarity is <span id="page_49"></span>
important to psychological research, much of which uses made-up problems.</div>
<div class="s4P">Since there is no outcome—and you probably did not even ask yourself whether there would ever be one—you were not trying to minimize error relative to that outcome. You tried to get the judgment right, to land on a number in which you had enough confidence to make it your answer. Of course, you were not perfectly confident in that answer, in the way you would be perfectly confident that four times six is twenty-four. You were aware of some uncertainty (and, as we will see, there is probably more of it than you recognized). But at some point, you decided that you were no longer making progress and settled for an answer.</div>
<div class="s4P">What made you feel you got the judgment right, or at least right enough to be your answer? We suggest this feeling is an <span class="s2WT-0">internal signal of judgment completion,</span>
 unrelated to any outside information. Your answer felt right if it seemed to fit comfortably enough with the evidence. An answer of 0 or 100 would not give you that sense of fit: the confidence it implies is inconsistent with the messy, ambiguous, conflicting evidence provided. But the number on which you settled, whatever it is, gave you the sense of coherence you needed. The aim of judgment, as you experienced it, was the achievement of a coherent solution.</div>
<div class="s4P">The essential feature of this internal signal is that the sense of coherence is part of the experience of judgment. It is not contingent on a real outcome. As a result, the internal signal is just as available for nonverifiable judgments as it is for real, verifiable ones. This explains why making a judgment about a fictitious character like Gambardi feels very much the same as does making a judgment about the real world.</div>
<div class="s7D">How Judgment Is Evaluated: The Outcome and the Process</div>
<div class="s4M">Verifiability does not change the experience of judgment as it takes place. It does, however, change its evaluation after the fact.</div>
<div class="s4P">Verifiable judgments can be scored by an objective observer on a simple measure of error: the difference between the judgment and the outcome. If a weather forecaster said today’s high <span id="page_50"></span>
temperature would be seventy degrees Fahrenheit and it is sixty-five degrees, the forecaster made an error of plus five degrees. Evidently, this approach does not work for nonverifiable judgments like the Gambardi problem, which have no true outcome. How, then, are we to decide what constitutes good judgment?</div>
<div class="s4P">The answer is that there is a second way to evaluate judgments. This approach applies both to verifiable and nonverifiable ones. It consists in evaluating the <span class="s2WT-0">process</span>
 of judgment. When we speak of good or bad judgments, we may be speaking either about the output (e.g., the number you produced in the Gambardi case) or about the process—what you did to arrive at that number.</div>
<div class="s4P">One approach to the evaluation of the process of judgment is to observe how that process performs when it is applied to a large number of cases. For instance, consider a political forecaster who has assigned probabilities of winning to a large number of candidates in local elections. He described one hundred of these candidates as being 70% likely to win. If seventy of them are eventually elected, we have a good indication of the forecaster’s skill in using the probability scale. The judgments are verifiable as an ensemble, although no single probability judgment can be declared right or wrong. Similarly, bias for or against a particular group can best be established by examining statistical results for a substantial number of cases.</div>
<div class="s4P">Another question that can be asked about the process of judgment is whether it conforms to the principles of logic or probability theory. A large body of research on cognitive biases of judgment has been in this vein.</div>
<div class="s4P">Focusing on the process of judgment, rather than its outcome, makes it possible to evaluate the quality of judgments that are not verifiable, such as judgments about fictitious problems or long-term forecasts. We may not be able to compare them to a known outcome, but we can still tell whether they have been made incorrectly. And when we turn to the question of <span class="s2WT-0">improving</span>
 judgments rather than just evaluating them, we will focus on process, too. All the procedures we recommend in this book to reduce bias and noise aim to adopt the judgment process that would minimize error over an ensemble of similar cases.</div>
<div class="s4P">We have contrasted two ways of evaluating a judgment: by comparing it to an <span class="s2WT-0">outcome</span>
 and by assessing the quality of the <span class="s2WT-0">process</span>
<span id="page_51"></span>
 that led to it. Note that when the judgment is verifiable, the two ways of evaluating it may reach different conclusions in a single case. A skilled and careful forecaster using the best possible tools and techniques will often miss the correct number in making a quarterly inflation forecast. Meanwhile, in a single quarter, a dart-throwing chimpanzee will sometimes be right.</div>
<div class="s4P">Scholars of decision-making offer clear advice to resolve this tension: focus on the process, not on the outcome of a single case. We recognize, however, that this is not standard practice in real life. Professionals are usually evaluated on how closely their judgments match verifiable outcomes, and if you ask them what they aim for in their judgments, a close match is what they will answer.</div>
<div class="s4P">In summary, what people usually claim to strive for in verifiable judgments is a prediction that matches the outcome. What they are effectively trying to achieve, regardless of verifiability, is the internal signal of completion provided by the coherence between the facts of the case and the judgment. And what they should be trying to achieve, normatively speaking, is the judgment process that would produce the best judgment over an ensemble of similar cases.</div>
<div class="s7D">Evaluative Judgments</div>
<div class="s4M">So far in this chapter, we have focused on predictive judgment tasks, and most of the judgments we will discuss are of that type. But <a href="part0008.xhtml">chapter 1</a>
, which discusses Judge Frankel and noise in sentencing by federal judges, examines another type of judgment. Sentencing a felon is not a prediction. It is an <span class="s2WT-0">evaluative judgment</span>
 that seeks to match the sentence to the severity of the crime. Judges at a wine fair and restaurant critics make evaluative judgments. Professors who grade essays, judges at ice-skating competitions, and committees that award grants to research projects make evaluative judgments.</div>
<div class="s4P">A different kind of evaluative judgment is made in decisions that involve multiple options and trade-offs between them. Consider managers who choose among candidates for hiring, management teams that must decide on strategic options, or even presidents choosing how to respond to an epidemic in Africa. To be sure, all these decisions rely on predictive judgments that provide input—for <span id="page_52"></span>
instance, how a candidate will perform in her first year, how the stock market will respond to a given strategic move, or how quickly the epidemic will spread if left unchecked. But the final decisions entail trade-offs between the pros and cons of various options, and these trade-offs are resolved by evaluative judgments.</div>
<div class="s4P">Like predictive judgments, evaluative judgments entail an expectation of bounded disagreement. No self-respecting federal judge is likely to say, “This is the punishment I like best, and I don’t care a bit if my colleagues think otherwise.” And decision makers who choose from several strategic options expect colleagues and observers who have the same information and share the same goals to agree with them, or at least not to disagree too much. Evaluative judgments partly depend on the values and preferences of those making them, but they are not mere matters of taste or opinion.</div>
<div class="s4P">For that reason, the boundary between predictive and evaluative judgments is fuzzy and people who make judgments are often unaware of it. Judges who set sentences or professors who grade essays think hard about their task and strive to find the “right” answer. They develop confidence in their judgments and in the justifications they have for them. Professionals feel much the same, act much the same, and speak much the same to justify themselves when their judgments are predictive (“How well will this new product sell?”) and when they are evaluative (“How well did my assistant perform this year?”).</div>
<div class="s7D">What’s Wrong with Noise</div>
<div class="s4M">The observation of noise in predictive judgments always indicates that something is wrong. If two doctors disagree on a diagnosis or two forecasters disagree about the next quarter’s sales, at least one of them must be in error. The error may happen because one of them is less skilled, and therefore more likely to be wrong, or because of some other source of noise. Regardless of the cause, failing to make the correct judgment can have serious consequences for those who rely on the diagnoses and forecasts of these individuals.</div>
<div class="s4P">Noise in evaluative judgments is problematic for a different reason. In any system in which judges are assumed to be interchangeable and assigned quasi-randomly, large disagreements <span id="page_53"></span>
about the same case violate expectations of fairness and consistency. If there are large differences in sentences given to the same defendant, we are in the domain of the “arbitrary cruelties” that Judge Frankel denounced. Even judges who believe in the value of individualized sentencing and who disagree on a robber’s sentence will agree that a level of disagreement that turns a judgment into a lottery is problematic. The same is true (if less dramatically so) when vastly different grades are given to the same essay, different safety ratings to the same restaurant, or different scores to the same ice-skater—or when one person, suffering from depression, gets social security disability benefits, while another person with the same condition gets nothing.</div>
<div class="s4P">Even when unfairness is only a minor concern, system noise poses another problem. People who are affected by evaluative judgments expect the values these judgments reflect to be those of the system, not of the individual judges. Something must have gone badly wrong if one customer, complaining of a defective laptop, gets fully reimbursed, and another gets a mere apology; or if one employee who has been with a firm for five years asks for a promotion and gets exactly that, while another employee, whose performance is otherwise identical, is politely turned down. System noise is inconsistency, and inconsistency damages the credibility of the system.</div>
<div class="s7D">Undesirable but Measurable</div>
<div class="s4M">All we need to measure noise is multiple judgments of the same problem. We do not need to know a true value. As the shooting-range story in the introduction illustrates, when we look at the back of the target, the bull’s-eye is invisible, but we can see the scatter of the shots. As soon as we know that all the shooters were aiming at the same bull’s-eye, we can measure noise. This is what a noise audit does. If we ask all our forecasters to estimate next quarter’s sales, the scatter in their forecasts is noise.</div>
<div class="s4P">This difference between bias and noise is essential for the practical purpose of improving judgments. It may seem paradoxical to claim that we can improve judgments when we cannot verify whether they are right. But we can—if we start by measuring noise. <span id="page_54"></span>
Regardless of whether the goal of judgment is just accuracy or a more complex trade-off between values, noise is undesirable and often measurable. And once noise is measured, as we will discuss in <a href="part0028.xhtml">part 5</a>
, it is often possible to reduce it.</div>
<div class="s84">Speaking of Professional Judgment</div>
<div class="s86">
<span class="class-2">“This is a matter of judgment. You can’t expect people to agree perfectly.”</span>
</div>
<div class="s88">
<span class="class-2">“Yes, this is a matter of judgment, but some judgments are so far out that they are wrong.”</span>
</div>
<div class="s88">
<span class="class-2">“Your choice between the candidates was just an expression of taste, not a serious judgment.”</span>
</div>
<div class="s8A">
<span class="class-2">“A decision requires both predictive and evaluative judgments.”</span>
</div>
</body>
</html>
