<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<title>part0035</title>
<link rel="stylesheet" type="text/css" href="stylesheet.css" />
</head>
<body>
<div id="page_279" class="s6B">
<a href="part0003.xhtml#a2XC">CHAPTER 24</a>
</div>
<div class="s6T">
<a href="part0003.xhtml#a2XC">Structure in Hiring</a>
</div>
<div class="s4M">
<span class="s2WY">I</span>
f you have ever held a job of any kind, the words <span class="s2WT-0">recruiting interview</span>
 might evoke some vivid and stressful memories. Job interviews, in which a candidate meets with a future supervisor or an HR professional, are a rite of passage required to enter many organizations.</div>
<div class="s4P">In most cases, interviews follow a well-rehearsed routine. After exchanging some pleasantries, interviewers ask candidates to describe their experience or elaborate on specific aspects of it. Questions are asked about achievements and challenges, motivations for the job, or improvement ideas for the company. Often the interviewers ask candidates to describe their personality and explain why they would be a good fit for the position or the company’s culture. Hobbies and interests are sometimes discussed. Toward the end, the candidate usually gets to ask a few questions, which are duly evaluated for relevance and insightfulness.</div>
<div class="s4P">If you are now in a position to hire employees, your selection methods probably include some version of this ritual. As one organizational psychologist noted, “It is rare, even unthinkable, for someone to be hired without some type of interview.” And almost all professionals rely to some degree on their intuitive judgments when making hiring decisions in these interviews.</div>
<div class="s4P">The ubiquity of the employment interview reflects a deep-seated belief in the value of judgment when it comes to choosing the people we will work with. And as a judgment task, personnel selection has a great advantage: because it is so ubiquitous and so important, organizational psychologists have studied it in great detail. The inaugural issue of the <span class="s2WT-0">Journal of Applied Psychology,</span>
 published in 1917, identified hiring as the “supreme problem… <span id="page_280"></span>
because human capacities are after all the chief national resources.” A century later, we know a lot about the effectiveness of various selection techniques (including standard interviews). No complex judgment task has been the focus of so much field research. This makes it a perfect test case, offering lessons that can be extrapolated to many judgments involving a choice among several options.</div>
<div class="s7D">The Dangers of Interviews</div>
<div class="s4M">If you are unfamiliar with research on the employment interview, what follows may surprise you. In essence, if your goal is to determine which candidates will succeed in a job and which will fail, standard interviews (also called unstructured interviews to distinguish them from structured interviews, to which we will turn shortly) are not very informative. To put it more starkly, they are often useless.</div>
<div class="s4P">To reach this conclusion, innumerable studies estimated the correlation between the rating an evaluator gives a candidate after an interview and the candidate’s eventual success on the job. If the correlation between the interview rating and success is high, then interviews—or any other recruiting techniques for which correlation is computed in the same manner—can be assumed to be a good predictor of how candidates will perform.</div>
<div class="s4P">A caveat is needed here. The definition of success is a nontrivial problem. Typically, performance is evaluated on the basis of supervisor ratings. Sometimes, the metric is length of employment. Such measures raise questions, of course, especially given the questionable validity of performance ratings, which we noted in the previous chapter. However, for the purpose of evaluating the quality of an employer’s judgments when selecting employees, it seems reasonable to use the judgments that the same employer makes when evaluating the employees thus hired. Any analysis of the quality of hiring decisions must make this assumption.</div>
<div class="s4P">So what do these analyses conclude? In <a href="part0020.xhtml">chapter 11</a>
, we mentioned a correlation between typical interview ratings and job performance ratings of .28. Other studies report correlations that range between .20 and .33. As we have seen, this is a very good <span id="page_281"></span>
correlation by social science standards—but not a very good one on which to base your decisions. Using the percent concordant (PC) we introduced in <a href="part0017.xhtml">part 3</a>
, we can calculate a probability: given the preceding levels of correlation, if all you know about two candidates is that one appeared better than the other in the interview, the chances that this candidate will indeed perform better are about 56 to 61%. Somewhat better than flipping a coin, for sure, but hardly a fail-safe way to make important decisions.</div>
<div class="s4P">Admittedly, interviews serve other purposes besides making a judgment about a candidate. Notably, they provide an opportunity to sell the company to promising candidates and to start building rapport with future colleagues. Yet from the perspective of an organization that invests time and effort in talent selection, the main purpose of interviews is clearly one of selection. And at that task, they are not exactly a terrific success.</div>
<div class="s7D">Noise in Interviewing</div>
<div class="s4M">We can easily see why traditional interviews produce error in their prediction of job performance. Some of this error has to do with what we have termed objective ignorance (see <a href="part0020.xhtml">chapter 11</a>
). Job performance depends on many things, including how quickly the person you hire adjusts to her new position or how various life events affect her work. Much of this is unpredictable at the time of hiring. This uncertainty limits the predictive validity of interviews and, indeed, any other personnel selection technique.</div>
<div class="s4P">Interviews are also a minefield of psychological biases. In recent years, people have become well aware that interviewers tend, often unintentionally, to favor candidates who are culturally similar to them or with whom they have something in common, including gender, race, and educational background. Many companies now recognize the risks posed by biases and try to address them through specific training of recruiting professionals and other employees. Other biases have also been known for decades. For instance, physical appearance plays a large part in the evaluation of candidates, even for positions where it should matter little or not at all. Such biases are shared by all or most recruiters and, when applied to a given candidate, will thus tend to produce a shared <span id="page_282"></span>
error—a negative or positive bias in the candidate’s evaluation.</div>
<div class="s4P">You will not be surprised to hear that there is noise as well: Different interviewers respond differently to the same candidate and reach different conclusions. Measures of the correlation between the ratings that two interviewers produce after interviewing the same candidate range between .37 and .44 (PC = 62–65%). One reason is that the candidate may not behave in exactly the same way with different interviewers. But even in panel interviews, where several interviewers are exposed to the same interviewee behavior, the correlation between their ratings is far from perfect. One meta-analysis estimates a correlation of .74 (PC = 76%). This means that you and another interviewer, after seeing the <span class="s2WT-0">same</span>
 two candidates in the <span class="s2WT-0">same</span>
 panel interview, will still disagree about which of two candidates is better about one-quarter of the time.</div>
<div class="s4P">This variability is largely the product of pattern noise, the difference in interviewers’ idiosyncratic reactions to a given interviewee. Most organizations fully expect this variability and, for that reason, require several interviewers to meet the same candidate, with the results aggregated in some way. (Typically, the aggregate opinion is formed through a discussion in which some sort of consensus must be reached—a procedure that creates its own problems, as we have already noted.)</div>
<div class="s4P">A more surprising finding is the presence of much occasion noise in interviews. There is strong evidence, for instance, that hiring recommendations are linked to impressions formed in the informal rapport-building phase of an interview, those first two or three minutes where you just chat amicably to put the candidate at ease. First impressions turn out to matter—a lot.</div>
<div class="s4P">Perhaps you think that judging on first impressions is unproblematic. At least some of what we learn from first impressions is meaningful. All of us know that we do learn something in the first seconds of interaction with a new acquaintance. It stands to reason that this may be particularly true of skilled interviewers. But the first seconds of an interview reflect exactly the sort of superficial qualities you associate with first impressions: early perceptions are based mostly on a candidate’s extraversion and verbal skills. Even the quality of a handshake is a significant predictor of hiring recommendations! We may all like a firm handshake, but few recruiters would consciously choose to make it a key hiring <span id="page_283"></span>
criterion.</div>
<div class="s7D">The Psychology of Interviewers</div>
<div class="s4M">Why do first impressions end up driving the outcome of a much longer interview? One reason is that in a traditional interview, interviewers are at liberty to steer the interview in the direction they see fit. They are likely to ask questions that confirm an initial impression. If a candidate seems shy and reserved, for instance, the interviewer may want to ask tough questions about the candidate’s past experiences of working in teams but perhaps will neglect to ask the same questions of someone who seems cheerful and gregarious. The evidence collected about these two candidates will not be the same. One study that tracked the behavior of interviewers who had formed a positive or negative initial impression from résumés and test scores found that initial impressions have a deep effect on the way the interview proceeds. Interviewers with positive first impressions, for instance, ask fewer questions and tend to “sell” the company to the candidate.</div>
<div class="s4P">The power of first impressions is not the only problematic aspect of interviews. Another is that as interviewers, we want the candidate sitting in front of us to <span class="s2WT-0">make sense</span>
 (a manifestation of our excessive tendency, discussed in <a href="part0023.xhtml">chapter 13</a>
, to seek and find coherence). In one striking experiment, researchers assigned students to play the role of interviewer or interviewee and told both that the interview should consist only of closed-ended, yes-or-no questions. They then asked some of the interviewees to answer questions <span class="s2WT-0">randomly</span>
. (The first letter of the questions as formulated determined if they should answer yes or no.) As the researchers wryly note, “Some of the interviewees were initially concerned that the random interview would break down and be revealed to be nonsense. No such problems occurred, and the interviews proceeded.” You read that right: <span class="s2WT-0">not a single interviewer</span>
 realized that the candidates were giving random answers. Worse, when asked to estimate whether they were “able to infer a lot about this person given the amount of time we spent together,” interviewers in this “random” condition were as likely to agree as those who had met candidates responding truthfully. Such is our ability to create <span id="page_284"></span>
coherence. As we can often find an imaginary pattern in random data or imagine a shape in the contours of a cloud, we are capable of finding logic in perfectly meaningless answers.</div>
<div class="s4P">For a less extreme illustration, consider the following case. One of the present authors had to interview a candidate who was, in his former position, chief financial officer at a midsize company. He noticed that the candidate had left this position after a few months and asked him why. The candidate explained that the reason was a “strategic disagreement with the CEO.” A colleague also interviewed the candidate, asked the same question, and got the same answer. In the debrief that followed, however, the two interviewers had radically different views. One, having so far formed a positive evaluation of the candidate, saw the candidate’s decision to leave the company as an indication of integrity and courage. The other, who had formed a negative first impression, construed the same fact as a sign of inflexibility, perhaps even of immaturity. The story illustrates that however much we would like to believe that our judgment about a candidate is based on facts, our interpretation of facts is colored by prior attitudes.</div>
<div class="s4P">The limitations of traditional interviews cast serious doubt on our ability to draw any meaningful conclusions from them. Yet impressions formed in an interview are vivid, and the interviewer is usually confident about them. When combining the conclusions reached in an interview with other cues about the candidate, we tend to give too much weight to the interview and too little to other data that may be more predictive, such as test scores.</div>
<div class="s4P">A story may help bring this observation to life. Professors who interview for a faculty position are often asked to teach in front of a panel of their peers to ensure that their teaching skills are up to the institution’s standards. It is, of course, a higher-stakes situation than an ordinary class. One of us once witnessed a candidate making a bad impression in this exercise, clearly because of the stress of the situation: the candidate’s résumé mentioned outstanding teaching evaluations and several awards for teaching excellence. Yet the vivid impression produced by his failure in one highly artificial situation weighed more heavily in the final decision than did the abstract data about his excellent past teaching performance.</div>
<div class="s4P">A final point: when interviews are not the only source of information about candidates—for instance, when there are also <span id="page_285"></span>
tests, references, or other inputs—these various inputs must be combined into an overall judgment. The question this raises is one you now recognize: should the inputs be combined using judgment (a clinical aggregation) or a formula (a mechanical aggregation)? As we saw in <a href="part0018.xhtml">chapter 9</a>
, the mechanical approach is superior both in general and in the specific case of work performance prediction. Unfortunately, surveys suggest that the overwhelming majority of HR professionals favor clinical aggregation. This practice adds yet another source of noise to an already-noisy process.</div>
<div class="s7D">Improving Personnel Selection Through Structure</div>
<div class="s4M">If traditional interviews and judgment-based hiring decisions have limited predictive validity, what can we do about them? Fortunately, research has also produced some advice on how to improve personnel selection, and some companies are paying attention.</div>
<div class="s4P">One example of a company that has upgraded its personnel selection practices and reported on the results is Google. Laszlo Bock, its former senior vice president of People Operations, tells the tale in his book <span class="s2WT-0">Work Rules!</span>
 Despite being focused on hiring talent of the highest caliber and devoting considerable resources to finding the right people, Google was struggling. An audit of the predictive validity of its recruiting interviews found “zero relationship (… ), a complete random mess.” The changes Google implemented to address this situation reflect principles that have emerged from decades of research. They also illustrate decision hygiene strategies.</div>
<div class="s4P">One of these strategies should be familiar by now: aggregation. Its use in this context is not a surprise. Almost all companies aggregate the judgments of multiple interviewers on the same candidate. Not to be outdone, Google sometimes had candidates suffer through twenty-five interviews! One of the conclusions of Bock’s review was to reduce that number to four, as he found that additional interviews added almost no predictive validity to what was achieved by the first four. To ensure this level of validity, however, Google stringently enforces a rule that not all companies observe: the company makes sure that the interviewers rate the candidate separately, <span class="s2WT-0">before</span>
 they communicate with one another. Once more: aggregation works—but only if the judgments <span id="page_286"></span>
are independent.</div>
<div class="s4P">Google also adopted a decision hygiene strategy we haven’t yet described in detail: <span class="s2WT-0">structuring complex judgments.</span>
 The term <span class="s2WT-0">structure</span>
 can mean many things. As we use the term here, a structured complex judgment is defined by three principles: decomposition, independence, and delayed holistic judgment.</div>
<div class="s4P">The first principle, <span class="s2WT-0">decomposition</span>
, breaks down the decision into components, or <span class="s2WT-0">mediating assessments.</span>
 This step serves the same purpose as the identification of the subjudgments in a guideline: it focuses the judges on the important cues. Decomposition acts as a road map to specify what data is needed. And it filters out irrelevant information.</div>
<div class="s4P">In Google’s case, there are four mediating assessments in the decomposition: general cognitive ability, leadership, cultural fit (called “googleyness”), and role-related knowledge. (Some of these assessments are then broken down into smaller components.) Note that a candidate’s good looks, smooth talk, exciting hobbies, and any other aspects, positive or negative, that a recruiter might notice in an unstructured interview are not on the list.</div>
<div class="s4P">Creating this sort of structure for a recruiting task may seem like mere common sense. Indeed, if you are hiring an entry-level accountant or an administrative assistant, standard job descriptions exist and specify the competencies needed. As professional recruiters know, however, defining the key assessments gets difficult for unusual or senior positions, and this step of definition is frequently overlooked. One prominent headhunter points out that defining the required competencies in a sufficiently specific manner is a challenging, often overlooked task. He highlights the importance for decision makers of “investing in the problem definition”: spending the necessary time up front, before you meet any candidates, to agree on a clear and detailed job description. The challenge here is that many interviewers use bloated job descriptions produced by consensus and compromise. The descriptions are vague wish lists of all the characteristics an ideal candidate would possess, and they offer no way to calibrate the characteristics or make trade-offs among them.</div>
<div class="s4P">The second principle of structured judgment, <span class="s2WT-0">independence,</span>
 requires that information on each assessment be collected independently. Just listing the components of the job description is <span id="page_287"></span>
not enough: most recruiters conducting traditional interviews also know the four or five things they look for in a candidate. The problem is that, in the conduct of the interview, they do not evaluate these elements separately. Each assessment influences the others, which makes each assessment very noisy.</div>
<div class="s4P">To overcome this problem, Google orchestrated ways to make assessments in a fact-based manner and independently of one another. Perhaps its most visible move was to introduce <span class="s2WT-0">structured behavioral interviews.</span>
 The interviewers’ task in such interviews is not to decide whether they like a candidate overall; it is to collect data about each assessment in the evaluation structure and to assign a score to the candidate on each assessment. To do so, interviewers are required to ask predefined questions about the candidate’s behaviors in past situations. They must also record the answers and score them against a predetermined rating scale, using a unified rubric. The rubric gives examples of what average, good, or great answers look like for each question. This shared scale (an example of the behaviorally anchored rating scales we introduced in the preceding chapter) helps reduce noise in judgments.</div>
<div class="s4P">If this approach sounds different from a traditional, chatty interview, it is. In fact, it can feel more like an exam or interrogation than a business encounter, and there is some evidence that both interviewees and interviewers dislike structured interviews (or at least prefer unstructured ones). There is continuing debate about exactly what an interview must include to qualify as structured. Still, one of the most consistent findings to emerge from the literature on interviewing is that structured interviews are far more predictive of future performance than are traditional, unstructured ones. Correlations with job performance range between .44 and .57. Using our PC metric, your chances of picking the better candidate with a structured interview are between 65 and 69%, a marked improvement over the 56 to 61% chance an unstructured interview would give you.</div>
<div class="s4P">Google uses other data as inputs on some of the dimensions it cares about. To test job-related knowledge, it relies in part on <span class="s2WT-0">work sample tests</span>
, such as asking a candidate for a programming job to write some code. Research has shown that work sample tests are among the best predictors of on-the-job performance. Google also uses “backdoor references,” supplied not by someone the candidate <span id="page_288"></span>
has nominated but by Google employees with whom the candidate has crossed paths.</div>
<div class="s4P">The third principle of structured judgment, <span class="s2WT-0">delayed holistic judgment,</span>
 can be summarized in a simple prescription: do not exclude intuition, but delay it. At Google, the final hiring recommendation is made collegially by a hiring committee, which reviews a complete file of all the ratings the candidates have obtained on each assessment in each interview and other relevant information in support of these assessments. On the basis of that information, the committee then decides whether to extend an offer.</div>
<div class="s4P">Despite the famously data-driven culture of this company, and despite all the evidence that a mechanical combination of data outperforms a clinical one, the final hiring decision is <span class="s2WT-0">not</span>
 mechanical. It remains a judgment, in which the committee takes all the evidence into account and weighs it holistically, engaging in a discussion of the question “Will this person be successful at Google?” The decision is not merely computed.</div>
<div class="s4P">In the next chapter, we will explain why we believe that this approach to making the final decision is a sensible one. But note that while they are not mechanical, Google’s final hiring decisions are anchored on the average score assigned by the four interviewers. They are also informed by the underlying evidence. In other words, Google allows judgment and intuition in its decision-making process only after all the evidence has been collected and analyzed. Thus, the tendency of each interviewer (and hiring committee member) to form quick, intuitive impressions and rush to judgment is kept in check.</div>
<div class="s4P">The three principles—once more, decomposition, independent assessment on each dimension, and delayed holistic judgment—do not necessarily provide a template for all organizations trying to improve their selection processes. But the principles are broadly consistent with the recommendations that organizational psychologists have formulated over the years. In fact, the principles bear some resemblance to the selection method that one of us (Kahneman) implemented in the Israeli army as early as 1956 and described in <span class="s2WT-0">Thinking, Fast and Slow.</span>
 That process, like the one Google put in place, formalized an evaluation structure (the list of personality and competence dimensions that had to be evaluated). It required interviewers to elicit objective evidence <span id="page_289"></span>
relevant to each dimension in turn and to score that dimension before moving on to the next. And it allowed recruiters to use judgment and intuition to reach a final decision—but only after the structured evaluation had taken place.</div>
<div class="s7X-1"><img src="rsrc31A.jpg" alt="" class="s7X-0" />
</div>
<div class="s7Z">There is overwhelming evidence of the superiority of structured judgment processes (including structured interviews) in hiring. Practical advice is available to guide executives who want to adopt them. As the example of Google illustrates and as other researchers have noted, structured judgment methods are also less costly—because few things are as costly as face time.</div>
<div class="s4P">Nevertheless, most executives remain convinced of the irreplaceable value of informal, interview-based methods. Remarkably, so do many candidates who believe that only a face-to-face interview will enable them to show a prospective employer their true mettle. Researchers have called this “the persistence of an illusion.” One thing is clear: recruiters and candidates severely underestimate the noise in hiring judgments.</div>
<div class="s84">Speaking of Structure in Hiring</div>
<div class="s86">
<span class="class-2">“In traditional, informal interviews, we often have an irresistible, intuitive feeling of understanding the candidate and knowing whether the person fits the bill. We must learn to distrust that feeling.”</span>
</div>
<div class="s88">
<span class="class-2">“Traditional interviews are dangerous not only because of biases but also because of noise.”</span>
</div>
<div class="s8A">
<span class="class-2">“We must add structure to our interviews and, more broadly, to our selection processes. Let’s start by defining much more clearly and specifically what we are looking for in candidates, and let’s make sure we evaluate the candidates independently on each of these dimensions.”</span>
</div>
</body>
</html>
